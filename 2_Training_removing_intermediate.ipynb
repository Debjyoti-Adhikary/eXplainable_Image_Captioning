{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.93s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.80s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                        | 2022/591753 [00:00<00:58, 10110.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 591753/591753 [00:56<00:00, 10549.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 64        # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False   # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1           # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 2048\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            (9,408)\n",
      "├─BatchNorm2d: 1-2                       (128)\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Bottleneck: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  (4,096)\n",
      "|    |    └─BatchNorm2d: 3-2             (128)\n",
      "|    |    └─Conv2d: 3-3                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-4             (128)\n",
      "|    |    └─Conv2d: 3-5                  (16,384)\n",
      "|    |    └─BatchNorm2d: 3-6             (512)\n",
      "|    |    └─ReLU: 3-7                    --\n",
      "|    |    └─Sequential: 3-8              (16,896)\n",
      "|    └─Bottleneck: 2-2                   --\n",
      "|    |    └─Conv2d: 3-9                  (16,384)\n",
      "|    |    └─BatchNorm2d: 3-10            (128)\n",
      "|    |    └─Conv2d: 3-11                 (36,864)\n",
      "|    |    └─BatchNorm2d: 3-12            (128)\n",
      "|    |    └─Conv2d: 3-13                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-14            (512)\n",
      "|    |    └─ReLU: 3-15                   --\n",
      "|    └─Bottleneck: 2-3                   --\n",
      "|    |    └─Conv2d: 3-16                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-17            (128)\n",
      "|    |    └─Conv2d: 3-18                 (36,864)\n",
      "|    |    └─BatchNorm2d: 3-19            (128)\n",
      "|    |    └─Conv2d: 3-20                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-21            (512)\n",
      "|    |    └─ReLU: 3-22                   --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Bottleneck: 2-4                   --\n",
      "|    |    └─Conv2d: 3-23                 (32,768)\n",
      "|    |    └─BatchNorm2d: 3-24            (256)\n",
      "|    |    └─Conv2d: 3-25                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-26            (256)\n",
      "|    |    └─Conv2d: 3-27                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-28            (1,024)\n",
      "|    |    └─ReLU: 3-29                   --\n",
      "|    |    └─Sequential: 3-30             (132,096)\n",
      "|    └─Bottleneck: 2-5                   --\n",
      "|    |    └─Conv2d: 3-31                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-32            (256)\n",
      "|    |    └─Conv2d: 3-33                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-34            (256)\n",
      "|    |    └─Conv2d: 3-35                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-36            (1,024)\n",
      "|    |    └─ReLU: 3-37                   --\n",
      "|    └─Bottleneck: 2-6                   --\n",
      "|    |    └─Conv2d: 3-38                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-39            (256)\n",
      "|    |    └─Conv2d: 3-40                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-41            (256)\n",
      "|    |    └─Conv2d: 3-42                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-43            (1,024)\n",
      "|    |    └─ReLU: 3-44                   --\n",
      "|    └─Bottleneck: 2-7                   --\n",
      "|    |    └─Conv2d: 3-45                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-46            (256)\n",
      "|    |    └─Conv2d: 3-47                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-48            (256)\n",
      "|    |    └─Conv2d: 3-49                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-50            (1,024)\n",
      "|    |    └─ReLU: 3-51                   --\n",
      "|    └─Bottleneck: 2-8                   --\n",
      "|    |    └─Conv2d: 3-52                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-53            (256)\n",
      "|    |    └─Conv2d: 3-54                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-55            (256)\n",
      "|    |    └─Conv2d: 3-56                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-57            (1,024)\n",
      "|    |    └─ReLU: 3-58                   --\n",
      "|    └─Bottleneck: 2-9                   --\n",
      "|    |    └─Conv2d: 3-59                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-60            (256)\n",
      "|    |    └─Conv2d: 3-61                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-62            (256)\n",
      "|    |    └─Conv2d: 3-63                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-64            (1,024)\n",
      "|    |    └─ReLU: 3-65                   --\n",
      "|    └─Bottleneck: 2-10                  --\n",
      "|    |    └─Conv2d: 3-66                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-67            (256)\n",
      "|    |    └─Conv2d: 3-68                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-69            (256)\n",
      "|    |    └─Conv2d: 3-70                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-71            (1,024)\n",
      "|    |    └─ReLU: 3-72                   --\n",
      "|    └─Bottleneck: 2-11                  --\n",
      "|    |    └─Conv2d: 3-73                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-74            (256)\n",
      "|    |    └─Conv2d: 3-75                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-76            (256)\n",
      "|    |    └─Conv2d: 3-77                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-78            (1,024)\n",
      "|    |    └─ReLU: 3-79                   --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Bottleneck: 2-12                  --\n",
      "|    |    └─Conv2d: 3-80                 262,144\n",
      "|    |    └─BatchNorm2d: 3-81            (1,024)\n",
      "|    |    └─Conv2d: 3-82                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-83            (1,024)\n",
      "|    |    └─Conv2d: 3-84                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-85            (4,096)\n",
      "|    |    └─ReLU: 3-86                   --\n",
      "|    |    └─Sequential: 3-87             (1,052,672)\n",
      "|    └─Bottleneck: 2-13                  --\n",
      "|    |    └─Conv2d: 3-88                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-89            (1,024)\n",
      "|    |    └─Conv2d: 3-90                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-91            (1,024)\n",
      "|    |    └─Conv2d: 3-92                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-93            (4,096)\n",
      "|    |    └─ReLU: 3-94                   --\n",
      "|    └─Bottleneck: 2-14                  --\n",
      "|    |    └─Conv2d: 3-95                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-96            (1,024)\n",
      "|    |    └─Conv2d: 3-97                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-98            (1,024)\n",
      "|    |    └─Conv2d: 3-99                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-100           (4,096)\n",
      "|    |    └─ReLU: 3-101                  --\n",
      "├─AdaptiveAvgPool2d: 1-8                 --\n",
      "=================================================================\n",
      "Total params: 16,219,200\n",
      "Trainable params: 262,144\n",
      "Non-trainable params: 15,957,056\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Conv2d: 1-1                            (9,408)\n",
      "├─BatchNorm2d: 1-2                       (128)\n",
      "├─ReLU: 1-3                              --\n",
      "├─MaxPool2d: 1-4                         --\n",
      "├─Sequential: 1-5                        --\n",
      "|    └─Bottleneck: 2-1                   --\n",
      "|    |    └─Conv2d: 3-1                  (4,096)\n",
      "|    |    └─BatchNorm2d: 3-2             (128)\n",
      "|    |    └─Conv2d: 3-3                  (36,864)\n",
      "|    |    └─BatchNorm2d: 3-4             (128)\n",
      "|    |    └─Conv2d: 3-5                  (16,384)\n",
      "|    |    └─BatchNorm2d: 3-6             (512)\n",
      "|    |    └─ReLU: 3-7                    --\n",
      "|    |    └─Sequential: 3-8              (16,896)\n",
      "|    └─Bottleneck: 2-2                   --\n",
      "|    |    └─Conv2d: 3-9                  (16,384)\n",
      "|    |    └─BatchNorm2d: 3-10            (128)\n",
      "|    |    └─Conv2d: 3-11                 (36,864)\n",
      "|    |    └─BatchNorm2d: 3-12            (128)\n",
      "|    |    └─Conv2d: 3-13                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-14            (512)\n",
      "|    |    └─ReLU: 3-15                   --\n",
      "|    └─Bottleneck: 2-3                   --\n",
      "|    |    └─Conv2d: 3-16                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-17            (128)\n",
      "|    |    └─Conv2d: 3-18                 (36,864)\n",
      "|    |    └─BatchNorm2d: 3-19            (128)\n",
      "|    |    └─Conv2d: 3-20                 (16,384)\n",
      "|    |    └─BatchNorm2d: 3-21            (512)\n",
      "|    |    └─ReLU: 3-22                   --\n",
      "├─Sequential: 1-6                        --\n",
      "|    └─Bottleneck: 2-4                   --\n",
      "|    |    └─Conv2d: 3-23                 (32,768)\n",
      "|    |    └─BatchNorm2d: 3-24            (256)\n",
      "|    |    └─Conv2d: 3-25                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-26            (256)\n",
      "|    |    └─Conv2d: 3-27                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-28            (1,024)\n",
      "|    |    └─ReLU: 3-29                   --\n",
      "|    |    └─Sequential: 3-30             (132,096)\n",
      "|    └─Bottleneck: 2-5                   --\n",
      "|    |    └─Conv2d: 3-31                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-32            (256)\n",
      "|    |    └─Conv2d: 3-33                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-34            (256)\n",
      "|    |    └─Conv2d: 3-35                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-36            (1,024)\n",
      "|    |    └─ReLU: 3-37                   --\n",
      "|    └─Bottleneck: 2-6                   --\n",
      "|    |    └─Conv2d: 3-38                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-39            (256)\n",
      "|    |    └─Conv2d: 3-40                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-41            (256)\n",
      "|    |    └─Conv2d: 3-42                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-43            (1,024)\n",
      "|    |    └─ReLU: 3-44                   --\n",
      "|    └─Bottleneck: 2-7                   --\n",
      "|    |    └─Conv2d: 3-45                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-46            (256)\n",
      "|    |    └─Conv2d: 3-47                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-48            (256)\n",
      "|    |    └─Conv2d: 3-49                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-50            (1,024)\n",
      "|    |    └─ReLU: 3-51                   --\n",
      "|    └─Bottleneck: 2-8                   --\n",
      "|    |    └─Conv2d: 3-52                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-53            (256)\n",
      "|    |    └─Conv2d: 3-54                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-55            (256)\n",
      "|    |    └─Conv2d: 3-56                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-57            (1,024)\n",
      "|    |    └─ReLU: 3-58                   --\n",
      "|    └─Bottleneck: 2-9                   --\n",
      "|    |    └─Conv2d: 3-59                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-60            (256)\n",
      "|    |    └─Conv2d: 3-61                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-62            (256)\n",
      "|    |    └─Conv2d: 3-63                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-64            (1,024)\n",
      "|    |    └─ReLU: 3-65                   --\n",
      "|    └─Bottleneck: 2-10                  --\n",
      "|    |    └─Conv2d: 3-66                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-67            (256)\n",
      "|    |    └─Conv2d: 3-68                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-69            (256)\n",
      "|    |    └─Conv2d: 3-70                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-71            (1,024)\n",
      "|    |    └─ReLU: 3-72                   --\n",
      "|    └─Bottleneck: 2-11                  --\n",
      "|    |    └─Conv2d: 3-73                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-74            (256)\n",
      "|    |    └─Conv2d: 3-75                 (147,456)\n",
      "|    |    └─BatchNorm2d: 3-76            (256)\n",
      "|    |    └─Conv2d: 3-77                 (65,536)\n",
      "|    |    └─BatchNorm2d: 3-78            (1,024)\n",
      "|    |    └─ReLU: 3-79                   --\n",
      "├─Sequential: 1-7                        --\n",
      "|    └─Bottleneck: 2-12                  --\n",
      "|    |    └─Conv2d: 3-80                 262,144\n",
      "|    |    └─BatchNorm2d: 3-81            (1,024)\n",
      "|    |    └─Conv2d: 3-82                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-83            (1,024)\n",
      "|    |    └─Conv2d: 3-84                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-85            (4,096)\n",
      "|    |    └─ReLU: 3-86                   --\n",
      "|    |    └─Sequential: 3-87             (1,052,672)\n",
      "|    └─Bottleneck: 2-13                  --\n",
      "|    |    └─Conv2d: 3-88                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-89            (1,024)\n",
      "|    |    └─Conv2d: 3-90                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-91            (1,024)\n",
      "|    |    └─Conv2d: 3-92                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-93            (4,096)\n",
      "|    |    └─ReLU: 3-94                   --\n",
      "|    └─Bottleneck: 2-14                  --\n",
      "|    |    └─Conv2d: 3-95                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-96            (1,024)\n",
      "|    |    └─Conv2d: 3-97                 (2,359,296)\n",
      "|    |    └─BatchNorm2d: 3-98            (1,024)\n",
      "|    |    └─Conv2d: 3-99                 (1,048,576)\n",
      "|    |    └─BatchNorm2d: 3-100           (4,096)\n",
      "|    |    └─ReLU: 3-101                  --\n",
      "├─AdaptiveAvgPool2d: 1-8                 --\n",
      "=================================================================\n",
      "Total params: 16,219,200\n",
      "Trainable params: 262,144\n",
      "Non-trainable params: 15,957,056\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "# %autoreload 2\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/9247], Loss: 4.0930, Perplexity: 59.91695\n",
      "Epoch [1/1], Step [200/9247], Loss: 4.4270, Perplexity: 83.67699\n",
      "Epoch [1/1], Step [300/9247], Loss: 3.5668, Perplexity: 35.40446\n",
      "Epoch [1/1], Step [400/9247], Loss: 3.0887, Perplexity: 21.9484\n",
      "Epoch [1/1], Step [500/9247], Loss: 3.6377, Perplexity: 38.0036\n",
      "Epoch [1/1], Step [600/9247], Loss: 2.9543, Perplexity: 19.1882\n",
      "Epoch [1/1], Step [700/9247], Loss: 2.9274, Perplexity: 18.6782\n",
      "Epoch [1/1], Step [800/9247], Loss: 2.9035, Perplexity: 18.2382\n",
      "Epoch [1/1], Step [900/9247], Loss: 2.9967, Perplexity: 20.0195\n",
      "Epoch [1/1], Step [1000/9247], Loss: 3.0025, Perplexity: 20.1352\n",
      "Epoch [1/1], Step [1100/9247], Loss: 2.8326, Perplexity: 16.9899\n",
      "Epoch [1/1], Step [1200/9247], Loss: 3.2603, Perplexity: 26.0583\n",
      "Epoch [1/1], Step [1300/9247], Loss: 2.7478, Perplexity: 15.6087\n",
      "Epoch [1/1], Step [1400/9247], Loss: 2.8822, Perplexity: 17.8540\n",
      "Epoch [1/1], Step [1500/9247], Loss: 2.6186, Perplexity: 13.7166\n",
      "Epoch [1/1], Step [1600/9247], Loss: 3.3824, Perplexity: 29.4428\n",
      "Epoch [1/1], Step [1700/9247], Loss: 2.7216, Perplexity: 15.2053\n",
      "Epoch [1/1], Step [1800/9247], Loss: 2.5249, Perplexity: 12.4895\n",
      "Epoch [1/1], Step [1900/9247], Loss: 2.6230, Perplexity: 13.7764\n",
      "Epoch [1/1], Step [2000/9247], Loss: 2.7240, Perplexity: 15.2417\n",
      "Epoch [1/1], Step [2100/9247], Loss: 2.8635, Perplexity: 17.5233\n",
      "Epoch [1/1], Step [2200/9247], Loss: 2.7265, Perplexity: 15.2796\n",
      "Epoch [1/1], Step [2300/9247], Loss: 2.7141, Perplexity: 15.0913\n",
      "Epoch [1/1], Step [2400/9247], Loss: 2.6293, Perplexity: 13.8639\n",
      "Epoch [1/1], Step [2500/9247], Loss: 2.7520, Perplexity: 15.6737\n",
      "Epoch [1/1], Step [2600/9247], Loss: 2.8029, Perplexity: 16.4925\n",
      "Epoch [1/1], Step [2700/9247], Loss: 3.0028, Perplexity: 20.1410\n",
      "Epoch [1/1], Step [2800/9247], Loss: 2.6047, Perplexity: 13.5267\n",
      "Epoch [1/1], Step [2900/9247], Loss: 3.0052, Perplexity: 20.1912\n",
      "Epoch [1/1], Step [3000/9247], Loss: 2.4681, Perplexity: 11.8003\n",
      "Epoch [1/1], Step [3100/9247], Loss: 2.6702, Perplexity: 14.4435\n",
      "Epoch [1/1], Step [3200/9247], Loss: 2.4239, Perplexity: 11.2903\n",
      "Epoch [1/1], Step [3300/9247], Loss: 2.4173, Perplexity: 11.2151\n",
      "Epoch [1/1], Step [3400/9247], Loss: 2.6912, Perplexity: 14.7499\n",
      "Epoch [1/1], Step [3500/9247], Loss: 2.6772, Perplexity: 14.5450\n",
      "Epoch [1/1], Step [3600/9247], Loss: 2.2697, Perplexity: 9.67650\n",
      "Epoch [1/1], Step [3648/9247], Loss: 2.4696, Perplexity: 11.8174"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 0 bytes free; 2.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bf584c86458b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Backward pass.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;31m# Update the parameters in the optimizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 116.00 MiB (GPU 0; 4.00 GiB total capacity; 1.80 GiB already allocated; 0 bytes free; 2.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "#             requests.request(\"POST\", \n",
    "#                              \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "#                              headers={'Authorization': \"STAR \" + response.text})\n",
    "        \n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'intermediate_rem_decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'intermediate_rem_encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del variables\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_l = [0,1,2,3,4,5,6,7,8]\n",
    "my_l.pop(6)\n",
    "print(my_l[5])\n",
    "print(my_l[6])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
