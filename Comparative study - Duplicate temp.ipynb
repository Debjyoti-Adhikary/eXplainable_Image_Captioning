{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "473b6eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from train import train_captioning_model\n",
    "from model_architecture import CNNtoRNN\n",
    "from gradcam import GradCAM\n",
    "import os\n",
    "from model import ConvNet\n",
    "from PIL import Image\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from utils import visualize_cam, Normalize, print_examples\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from datetime import datetime\n",
    "from get_loader import get_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a2069cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "preprocessing = transforms.Compose([\n",
    "            transforms.Resize((356,356)),\n",
    "            transforms.RandomCrop((299,299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "        ])\n",
    "\n",
    "train_loader, val_loader, dataset = get_loader(\n",
    "        root_folder=\"flickr8k/Images\",\n",
    "        annotation_file=\"flickr8k/captions.txt\",\n",
    "        transform = preprocessing,\n",
    "        num_workers=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4b17a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saliency_and_caption(file_names, model,filename):\n",
    "    salient_image_addresses = []\n",
    "    predicted_captions = []\n",
    "    for file_name in file_names:\n",
    "        image_path = \"images\\\\\" + file_name\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "        test_img = transform(image.convert(\"RGB\")).unsqueeze(0) #\n",
    "        predicted_caption = model.caption_image(test_img.cuda(), dataset.vocab)\n",
    "        predicted_captions.append(predicted_caption)\n",
    "    \n",
    "        for name, param in model.CNN.CNNArchitecture.named_parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        torch_img = torch.from_numpy(np.asarray(image)).permute(2, 0, 1).unsqueeze(0).float().div(255).cuda()\n",
    "        torch_img = F.upsample(torch_img, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        norm_torch_img = normalizer(torch_img)\n",
    "\n",
    "        images = []\n",
    "\n",
    "        for layer,_ in model.CNN.CNNArchitecture.named_parameters():\n",
    "            if 'bias' not in layer and 'classifier' not in layer:\n",
    "                layer_name_list = layer.split('.')\n",
    "                layer_name = layer_name_list[0]+'_'+layer_name_list[1]\n",
    "                cam_dict = dict()\n",
    "                model_dict = dict(type='alexnet',arch=model.CNN.CNNArchitecture.cuda(), layer_name=layer_name,input_size=(224,224))\n",
    "                model_GradCAM = GradCAM(model_dict, True)\n",
    "                mask , logit = model_GradCAM(torch_img)\n",
    "                mask = mask.cpu()\n",
    "                heatmap_t, result_t = visualize_cam(mask,torch_img)\n",
    "                layer_Image = torch.stack([torch_img.squeeze().cpu(), heatmap_t, result_t], 0)\n",
    "                images.append(layer_Image)\n",
    "        images = make_grid(torch.cat(images, 0), nrow=3)\n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S_\")\n",
    "\n",
    "        output_dir = 'outputs'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        output_name = dt_string + file_name\n",
    "        output_path = os.path.join(output_dir,output_name)\n",
    "        save_image(images, output_path)\n",
    "\n",
    "        salient_image_addresses.append(output_path)\n",
    "    name_dict = {\n",
    "            'captions': predicted_captions,\n",
    "            'images': salient_image_addresses\n",
    "          }\n",
    "\n",
    "    df = pd.DataFrame(name_dict)\n",
    "    df.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4feffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [\"girl_car.jpg\",\"girl_water.jpg\",\"car.jpg\",\"dog.jpg\",\"man_fish.jpg\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051007e7",
   "metadata": {},
   "source": [
    "FULL NETWORK TRAINING (5 conv layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7275883",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512 #\n",
    "num_epochs = 100\n",
    "visual_model = models.alexnet(pretrained=True) #pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d1ec69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model.classifier = nn.Sequential(nn.Linear(9216,embed_size))\n",
    "\n",
    "for name,param in visual_model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f72d99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 3.0358011722564697 Step : 313 Train Accuracy : 61.737179098738125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.6955652236938477 Average Valid Accuracy : 0.9251691997051239\n"
     ]
    }
   ],
   "source": [
    "alexnet_full_model = train_captioning_model(embed_size,visual_model,num_epochs, load_model=False, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34c228ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\functional.py:3328: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "C:\\Users\\ACER\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "filename=\"full_model_result.csv\"\n",
    "saliency_and_caption(test_images,alexnet_full_model,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60def55",
   "metadata": {},
   "source": [
    "4 LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f785b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model.classifier = nn.Sequential(nn.Linear(9216,embed_size))\n",
    "visual_model.features = nn.Sequential(*[visual_model.features[i] for i in range(10)])\n",
    "\n",
    "for name,param in visual_model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5d3a72db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 2.939314126968384 Step : 313 Train Accuracy : 62.52790745749371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 3.0162580013275146 Average Valid Accuracy : 0.9823007434606552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 2.711622714996338 Step : 626 Train Accuracy : 69.66637614369392\n",
      "Average loss on Validation Set : 3.0162580013275146 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 Loss : 2.689408779144287 Step : 939 Train Accuracy : 72.21162861585617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.6272077560424805 Average Valid Accuracy : 1.0954109132289886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 Loss : 2.553788185119629 Step : 1252 Train Accuracy : 74.04748938977718\n",
      "Average loss on Validation Set : 2.6272077560424805 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 Loss : 2.382962942123413 Step : 1565 Train Accuracy : 76.12488314509392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.2935900688171387 Average Valid Accuracy : 1.1930032968521118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "alexnet_model_4 = train_captioning_model(embed_size,visual_model,num_epochs, load_model=False, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5618b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"4_layers_result.csv\"\n",
    "saliency_and_caption(test_images,alexnet_model_4,filename)\n",
    "# predicted_captions_4, layered_saliency_images_4 = saliency_and_caption(test_images,alexnet_model_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824b552",
   "metadata": {},
   "source": [
    "3 CONV LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f06cc2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model.classifier = nn.Sequential(nn.Linear(13824,embed_size))\n",
    "visual_model.features = nn.Sequential(*[visual_model.features[i] for i in range(8)])\n",
    "\n",
    "for name,param in visual_model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5efe7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 2.991016387939453 Step : 313 Train Accuracy : 61.95760487020016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.9707908630371094 Average Valid Accuracy : 0.8943258076906204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 2.781543254852295 Step : 626 Train Accuracy : 69.2215428352356\n",
      "Average loss on Validation Set : 2.9707908630371094 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 Loss : 2.6379730701446533 Step : 939 Train Accuracy : 71.62228222191334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.6216704845428467 Average Valid Accuracy : 1.0958247482776642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 Loss : 2.523810863494873 Step : 1252 Train Accuracy : 74.18602454662323\n",
      "Average loss on Validation Set : 2.6216704845428467 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 Loss : 2.4719078540802 Step : 1565 Train Accuracy : 75.81526413559914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.2977919578552246 Average Valid Accuracy : 1.0781202614307404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "alexnet_model_3 = train_captioning_model(embed_size,visual_model,num_epochs, load_model=False, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53aa0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"3_layers_result.csv\"\n",
    "saliency_and_caption(test_images,alexnet_model_3,filename)\n",
    "# predicted_captions_3, layered_saliency_images_3 = saliency_and_caption(test_images,alexnet_model_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4de565b",
   "metadata": {},
   "source": [
    "2 LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d978783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_model.classifier = nn.Sequential(nn.Linear(6912,embed_size))\n",
    "visual_model.features = nn.Sequential(*[visual_model.features[i] for i in range(6)])\n",
    "\n",
    "for name,param in visual_model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51b8b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 3.091157913208008 Step : 313 Train Accuracy : 61.924809485673904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 3.073967218399048 Average Valid Accuracy : 0.8495033532381058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 2.7200350761413574 Step : 626 Train Accuracy : 69.27134282886982\n",
      "Average loss on Validation Set : 3.073967218399048 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 Loss : 2.7014474868774414 Step : 939 Train Accuracy : 71.72322028875351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.72670841217041 Average Valid Accuracy : 0.8615242838859558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 Loss : 2.462919235229492 Step : 1252 Train Accuracy : 73.4545606225729\n",
      "Average loss on Validation Set : 2.72670841217041 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 Loss : 2.3691978454589844 Step : 1565 Train Accuracy : 74.84238375723362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.6706299781799316 Average Valid Accuracy : 0.954964205622673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "alexnet_model_2 = train_captioning_model(embed_size,visual_model,num_epochs, load_model=False, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7124b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename=\"2_layers_result.csv\"\n",
    "saliency_and_caption(test_images,alexnet_model_2,filename)\n",
    "# predicted_captions_2, layered_saliency_images_2 = saliency_and_caption(test_images,alexnet_model_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ace58",
   "metadata": {},
   "source": [
    "1 LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "71667ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "40051\n",
      "404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0 Loss : 3.1294779777526855 Step : 313 Train Accuracy : 61.829709273763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 3.1412336826324463 Average Valid Accuracy : 0.8848900347948074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 Loss : 2.8299026489257812 Step : 626 Train Accuracy : 69.11363899707794\n",
      "Average loss on Validation Set : 3.1412336826324463 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2 Loss : 2.6615285873413086 Step : 939 Train Accuracy : 71.21358519792557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.723855972290039 Average Valid Accuracy : 0.930815264582634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3 Loss : 2.69767165184021 Step : 1252 Train Accuracy : 72.78073942661285\n",
      "Average loss on Validation Set : 2.723855972290039 Average Valid Accuracy : 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4 Loss : 2.5773074626922607 Step : 1565 Train Accuracy : 73.76449097692966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss on Validation Set : 2.868346929550171 Average Valid Accuracy : 0.9269476234912872\n"
     ]
    }
   ],
   "source": [
    "visual_model.classifier = nn.Sequential(nn.Linear(2304,embed_size))\n",
    "visual_model.features = nn.Sequential(*[visual_model.features[i] for i in range(3)])\n",
    "\n",
    "for name,param in visual_model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "alexnet_model_1 = train_captioning_model(embed_size,visual_model,num_epochs, load_model=False, save_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd70c93c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "└─Linear: 0-1                            (2,049,000)\n",
      "=================================================================\n",
      "Total params: 2,049,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,049,000\n",
      "=================================================================\n",
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "└─Linear: 0-1                            (2,049,000)\n",
      "=================================================================\n",
      "Total params: 2,049,000\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,049,000\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad_(False)\n",
    "print(resnet.fc.in_features)\n",
    "print(summary(resnet.fc))\n",
    "# self.resnet = nn.Sequential(*modules)\n",
    "# self.embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "# self.init_weights()\n",
    "# predicted_captions_1, layered_saliency_images_1 = saliency_and_caption(test_images,alexnet_model_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
