{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930c540d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.83s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                        | 1066/591753 [00:00<00:55, 10579.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 591753/591753 [00:50<00:00, 11755.41it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "sys.path.append('cocoapi\\PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# TODO #1: Define a transform to pre-process the testing images.\n",
    "transform_test = transforms.Compose([ \n",
    "    transforms.Resize(256),                          \n",
    "    transforms.CenterCrop(224),                             \n",
    "    transforms.ToTensor(),                           \n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "#-#-#-# Do NOT modify the code below this line. #-#-#-#\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Create the data loader.\n",
    "data_loader = get_loader(transform=transform_train,    \n",
    "                         mode='train')\n",
    "vocab_size = len(data_loader.dataset.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566cd72b",
   "metadata": {},
   "source": [
    "IMPORTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f72e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2048\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from training_incremental_model import Incremental_EncoderCNN, Incremental_DecoderRNN\n",
    "import math\n",
    "\n",
    "batch_size = 64 #32 #64        # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False   # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1           # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'   \n",
    "\n",
    "encoder = Incremental_EncoderCNN(embed_size)\n",
    "decoder = Incremental_DecoderRNN(embed_size,hidden_size,vocab_size)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# data_loader = get_loader(transform=transform_train,\n",
    "#                          mode='train',\n",
    "#                          batch_size=batch_size,\n",
    "#                          vocab_threshold=vocab_threshold,\n",
    "#                          vocab_from_file=vocab_from_file)\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e63aade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace=True),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "myModel = models.resnet152(pretrained=True)\n",
    "modules_list = list(myModel.children())[:-1]\n",
    "modules_list.pop(7)\n",
    "modules_list.pop(6)\n",
    "modules_list.pop(5)\n",
    "modules_list.pop(4)\n",
    "modules_list.pop(3)\n",
    "modules_list\n",
    "# print(len(modules_list))\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[-2])\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[7])\n",
    "# print(\"----------------------------------------------\")\n",
    "# modules_list.pop(7)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[-2])\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[6])\n",
    "# print(\"----------------------------------------------\")\n",
    "# modules_list.pop(6)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[-2])\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[5])\n",
    "# print(\"----------------------------------------------\")\n",
    "# modules_list.pop(5)\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[-2])\n",
    "# print(\"----------------------------------------------\")\n",
    "# print(modules_list[4])\n",
    "# print(\"----------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "994395a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
       " BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n",
       " Sequential(\n",
       "   (0): Bottleneck(\n",
       "     (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "     (downsample): Sequential(\n",
       "       (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "       (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     )\n",
       "   )\n",
       "   (1): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "   )\n",
       "   (2): Bottleneck(\n",
       "     (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "     (relu): ReLU(inplace=True)\n",
       "   )\n",
       " ),\n",
       " AdaptiveAvgPool2d(output_size=(1, 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modules_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef333ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=3.21s)\n",
      "creating index...\n",
      "index created!\n",
      "[0/591753] Tokenizing captions...\n",
      "[100000/591753] Tokenizing captions...\n",
      "[200000/591753] Tokenizing captions...\n",
      "[300000/591753] Tokenizing captions...\n",
      "[400000/591753] Tokenizing captions...\n",
      "[500000/591753] Tokenizing captions...\n",
      "loading annotations into memory...\n",
      "Done (t=0.76s)\n",
      "creating index...\n",
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 591753/591753 [00:50<00:00, 11697.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 2048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 32 #32 #64        # batch size\n",
    "vocab_threshold = 5        # minimum word count threshold\n",
    "vocab_from_file = False   # if True, load existing vocab file\n",
    "embed_size = 512           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 1           # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "encoder = Incremental_EncoderCNN(embed_size)\n",
    "decoder = Incremental_DecoderRNN(embed_size, hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22513556",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f6bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch [1/1], Step [100/591753], Loss: 2.3304, Perplexity: 10.2821\n",
      "Epoch [1/1], Step [200/591753], Loss: 0.9635, Perplexity: 2.620961\n",
      "Epoch [1/1], Step [300/591753], Loss: 3.0925, Perplexity: 22.03202\n",
      "Epoch [1/1], Step [400/591753], Loss: 1.3053, Perplexity: 3.688775\n",
      "Epoch [1/1], Step [500/591753], Loss: 2.4636, Perplexity: 11.74669\n",
      "Epoch [1/1], Step [600/591753], Loss: 2.0749, Perplexity: 7.963728\n",
      "Epoch [1/1], Step [700/591753], Loss: 1.9126, Perplexity: 6.770533\n",
      "Epoch [1/1], Step [800/591753], Loss: 1.8255, Perplexity: 6.206069\n",
      "Epoch [1/1], Step [900/591753], Loss: 1.4804, Perplexity: 4.394608\n",
      "Epoch [1/1], Step [1000/591753], Loss: 6.4259, Perplexity: 617.6615\n",
      "Epoch [1/1], Step [1100/591753], Loss: 2.9898, Perplexity: 19.88198\n",
      "Epoch [1/1], Step [1200/591753], Loss: 2.3762, Perplexity: 10.76401\n",
      "Epoch [1/1], Step [1300/591753], Loss: 1.9771, Perplexity: 7.221945\n",
      "Epoch [1/1], Step [1400/591753], Loss: 2.1177, Perplexity: 8.31237\n",
      "Epoch [1/1], Step [1500/591753], Loss: 1.9289, Perplexity: 6.882048\n",
      "Epoch [1/1], Step [1600/591753], Loss: 2.9153, Perplexity: 18.45468\n",
      "Epoch [1/1], Step [1700/591753], Loss: 2.0529, Perplexity: 7.790573\n",
      "Epoch [1/1], Step [1800/591753], Loss: 2.3528, Perplexity: 10.51467\n",
      "Epoch [1/1], Step [1900/591753], Loss: 4.2589, Perplexity: 70.73379\n",
      "Epoch [1/1], Step [2000/591753], Loss: 2.3233, Perplexity: 10.20974\n",
      "Epoch [1/1], Step [2100/591753], Loss: 1.8963, Perplexity: 6.661421\n",
      "Epoch [1/1], Step [2200/591753], Loss: 1.1720, Perplexity: 3.22830\n",
      "Epoch [1/1], Step [2300/591753], Loss: 2.4985, Perplexity: 12.16458\n",
      "Epoch [1/1], Step [2400/591753], Loss: 5.1510, Perplexity: 172.6048\n",
      "Epoch [1/1], Step [2500/591753], Loss: 2.1670, Perplexity: 8.73180\n",
      "Epoch [1/1], Step [2600/591753], Loss: 2.6653, Perplexity: 14.37218\n",
      "Epoch [1/1], Step [2700/591753], Loss: 4.7095, Perplexity: 110.9916\n",
      "Epoch [1/1], Step [2800/591753], Loss: 1.3817, Perplexity: 3.98160\n",
      "Epoch [1/1], Step [2900/591753], Loss: 1.7666, Perplexity: 5.851253\n",
      "Epoch [1/1], Step [3000/591753], Loss: 2.9471, Perplexity: 19.05021\n",
      "Epoch [1/1], Step [3100/591753], Loss: 2.0896, Perplexity: 8.081979\n",
      "Epoch [1/1], Step [3200/591753], Loss: 3.8758, Perplexity: 48.22120\n",
      "Epoch [1/1], Step [3300/591753], Loss: 3.2036, Perplexity: 24.62024\n",
      "Epoch [1/1], Step [3400/591753], Loss: 3.3673, Perplexity: 28.99987\n",
      "Epoch [1/1], Step [3500/591753], Loss: 1.7525, Perplexity: 5.768887\n",
      "Epoch [1/1], Step [3600/591753], Loss: 1.5297, Perplexity: 4.616835\n",
      "Epoch [1/1], Step [3700/591753], Loss: 3.5706, Perplexity: 35.53855\n",
      "Epoch [1/1], Step [3800/591753], Loss: 3.3900, Perplexity: 29.66591\n",
      "Epoch [1/1], Step [3900/591753], Loss: 1.8242, Perplexity: 6.197820\n",
      "Epoch [1/1], Step [4000/591753], Loss: 2.2713, Perplexity: 9.692263\n",
      "Epoch [1/1], Step [4100/591753], Loss: 3.6419, Perplexity: 38.16255\n",
      "Epoch [1/1], Step [4200/591753], Loss: 2.4752, Perplexity: 11.88420\n",
      "Epoch [1/1], Step [4300/591753], Loss: 2.7381, Perplexity: 15.45830\n",
      "Epoch [1/1], Step [4400/591753], Loss: 2.7301, Perplexity: 15.33506\n",
      "Epoch [1/1], Step [4500/591753], Loss: 2.5861, Perplexity: 13.27855\n",
      "Epoch [1/1], Step [4600/591753], Loss: 3.4030, Perplexity: 30.05414\n",
      "Epoch [1/1], Step [4700/591753], Loss: 2.2826, Perplexity: 9.802254\n",
      "Epoch [1/1], Step [4800/591753], Loss: 2.3436, Perplexity: 10.41832\n",
      "Epoch [1/1], Step [4900/591753], Loss: 2.6043, Perplexity: 13.5217\n",
      "Epoch [1/1], Step [5000/591753], Loss: 1.5902, Perplexity: 4.904626\n",
      "Epoch [1/1], Step [5100/591753], Loss: 2.7958, Perplexity: 16.3757\n",
      "Epoch [1/1], Step [5200/591753], Loss: 2.2976, Perplexity: 9.950293\n",
      "Epoch [1/1], Step [5300/591753], Loss: 1.9810, Perplexity: 7.25014\n",
      "Epoch [1/1], Step [5400/591753], Loss: 2.6745, Perplexity: 14.50573\n",
      "Epoch [1/1], Step [5500/591753], Loss: 1.5576, Perplexity: 4.747630\n",
      "Epoch [1/1], Step [5600/591753], Loss: 1.0375, Perplexity: 2.822285\n",
      "Epoch [1/1], Step [5700/591753], Loss: 2.4343, Perplexity: 11.40814\n",
      "Epoch [1/1], Step [5800/591753], Loss: 3.5742, Perplexity: 35.6666\n",
      "Epoch [1/1], Step [5900/591753], Loss: 3.4050, Perplexity: 30.11478\n",
      "Epoch [1/1], Step [6000/591753], Loss: 2.7461, Perplexity: 15.58149\n",
      "Epoch [1/1], Step [6100/591753], Loss: 1.7562, Perplexity: 5.79031\n",
      "Epoch [1/1], Step [6200/591753], Loss: 2.3882, Perplexity: 10.8943\n",
      "Epoch [1/1], Step [6300/591753], Loss: 2.2172, Perplexity: 9.181328\n",
      "Epoch [1/1], Step [6400/591753], Loss: 1.6495, Perplexity: 5.204280\n",
      "Epoch [1/1], Step [6500/591753], Loss: 1.6814, Perplexity: 5.37297\n",
      "Epoch [1/1], Step [6600/591753], Loss: 2.2273, Perplexity: 9.274658\n",
      "Epoch [1/1], Step [6700/591753], Loss: 2.5995, Perplexity: 13.4571\n",
      "Epoch [1/1], Step [6800/591753], Loss: 2.6523, Perplexity: 14.18637\n",
      "Epoch [1/1], Step [6900/591753], Loss: 2.9305, Perplexity: 18.7373\n",
      "Epoch [1/1], Step [7000/591753], Loss: 2.4138, Perplexity: 11.17607\n",
      "Epoch [1/1], Step [7100/591753], Loss: 2.1888, Perplexity: 8.924131\n",
      "Epoch [1/1], Step [7200/591753], Loss: 3.1035, Perplexity: 22.27495\n",
      "Epoch [1/1], Step [7300/591753], Loss: 2.5904, Perplexity: 13.33589\n",
      "Epoch [1/1], Step [7400/591753], Loss: 2.7147, Perplexity: 15.10017\n",
      "Epoch [1/1], Step [7500/591753], Loss: 2.3906, Perplexity: 10.92019\n",
      "Epoch [1/1], Step [7600/591753], Loss: 2.0376, Perplexity: 7.672237\n",
      "Epoch [1/1], Step [7700/591753], Loss: 2.4375, Perplexity: 11.44461\n",
      "Epoch [1/1], Step [7800/591753], Loss: 2.6145, Perplexity: 13.66038\n",
      "Epoch [1/1], Step [7900/591753], Loss: 1.8139, Perplexity: 6.134253\n",
      "Epoch [1/1], Step [8000/591753], Loss: 2.6658, Perplexity: 14.37933\n",
      "Epoch [1/1], Step [8100/591753], Loss: 1.8496, Perplexity: 6.357258\n",
      "Epoch [1/1], Step [8200/591753], Loss: 2.3424, Perplexity: 10.40581\n",
      "Epoch [1/1], Step [8300/591753], Loss: 2.5322, Perplexity: 12.58122\n",
      "Epoch [1/1], Step [8400/591753], Loss: 2.4730, Perplexity: 11.858184\n",
      "Epoch [1/1], Step [8500/591753], Loss: 2.5150, Perplexity: 12.3664\n",
      "Epoch [1/1], Step [8600/591753], Loss: 1.5601, Perplexity: 4.759332\n",
      "Epoch [1/1], Step [8700/591753], Loss: 3.0436, Perplexity: 20.98013\n",
      "Epoch [1/1], Step [8800/591753], Loss: 1.8925, Perplexity: 6.63619\n",
      "Epoch [1/1], Step [8900/591753], Loss: 2.6654, Perplexity: 14.37381\n",
      "Epoch [1/1], Step [9000/591753], Loss: 4.0795, Perplexity: 59.11561\n",
      "Epoch [1/1], Step [9100/591753], Loss: 2.3334, Perplexity: 10.31300\n",
      "Epoch [1/1], Step [9200/591753], Loss: 2.7385, Perplexity: 15.46340\n",
      "Epoch [1/1], Step [9300/591753], Loss: 2.2336, Perplexity: 9.333444\n",
      "Epoch [1/1], Step [9400/591753], Loss: 1.4143, Perplexity: 4.113500\n",
      "Epoch [1/1], Step [9500/591753], Loss: 1.8173, Perplexity: 6.15502\n",
      "Epoch [1/1], Step [9600/591753], Loss: 1.2077, Perplexity: 3.345795\n",
      "Epoch [1/1], Step [9700/591753], Loss: 3.1882, Perplexity: 24.24424\n",
      "Epoch [1/1], Step [9800/591753], Loss: 2.2835, Perplexity: 9.811364\n",
      "Epoch [1/1], Step [9900/591753], Loss: 2.7095, Perplexity: 15.02220\n",
      "Epoch [1/1], Step [10000/591753], Loss: 3.2695, Perplexity: 26.2987\n",
      "Epoch [1/1], Step [10100/591753], Loss: 2.5206, Perplexity: 12.43594\n",
      "Epoch [1/1], Step [10200/591753], Loss: 2.7920, Perplexity: 16.31333\n",
      "Epoch [1/1], Step [10300/591753], Loss: 1.7858, Perplexity: 5.964583\n",
      "Epoch [1/1], Step [10400/591753], Loss: 2.6184, Perplexity: 13.71431\n",
      "Epoch [1/1], Step [10500/591753], Loss: 2.7804, Perplexity: 16.12607\n",
      "Epoch [1/1], Step [10600/591753], Loss: 2.7811, Perplexity: 16.13637\n",
      "Epoch [1/1], Step [10700/591753], Loss: 1.2799, Perplexity: 3.596272\n",
      "Epoch [1/1], Step [10800/591753], Loss: 2.1117, Perplexity: 8.26217\n",
      "Epoch [1/1], Step [10900/591753], Loss: 3.9420, Perplexity: 51.52288\n",
      "Epoch [1/1], Step [11000/591753], Loss: 1.8617, Perplexity: 6.43497\n",
      "Epoch [1/1], Step [11100/591753], Loss: 2.2578, Perplexity: 9.561689\n",
      "Epoch [1/1], Step [11200/591753], Loss: 2.8424, Perplexity: 17.15698\n",
      "Epoch [1/1], Step [11300/591753], Loss: 1.7881, Perplexity: 5.978121\n",
      "Epoch [1/1], Step [11400/591753], Loss: 3.4621, Perplexity: 31.8850\n",
      "Epoch [1/1], Step [11500/591753], Loss: 2.4066, Perplexity: 11.09599\n",
      "Epoch [1/1], Step [11600/591753], Loss: 3.0583, Perplexity: 21.29188\n",
      "Epoch [1/1], Step [11700/591753], Loss: 2.7547, Perplexity: 15.7159\n",
      "Epoch [1/1], Step [11800/591753], Loss: 2.7368, Perplexity: 15.4379\n",
      "Epoch [1/1], Step [11900/591753], Loss: 5.5494, Perplexity: 257.0956\n",
      "Epoch [1/1], Step [12000/591753], Loss: 3.0201, Perplexity: 20.49281\n",
      "Epoch [1/1], Step [12100/591753], Loss: 1.4018, Perplexity: 4.062515\n",
      "Epoch [1/1], Step [12200/591753], Loss: 2.4914, Perplexity: 12.07870\n",
      "Epoch [1/1], Step [12300/591753], Loss: 1.1023, Perplexity: 3.011195\n",
      "Epoch [1/1], Step [12400/591753], Loss: 1.7939, Perplexity: 6.012905\n",
      "Epoch [1/1], Step [12500/591753], Loss: 1.7925, Perplexity: 6.00420\n",
      "Epoch [1/1], Step [12600/591753], Loss: 3.4527, Perplexity: 31.58702\n",
      "Epoch [1/1], Step [12700/591753], Loss: 1.4421, Perplexity: 4.229579\n",
      "Epoch [1/1], Step [12800/591753], Loss: 1.3450, Perplexity: 3.838310\n",
      "Epoch [1/1], Step [12900/591753], Loss: 3.2895, Perplexity: 26.83026\n",
      "Epoch [1/1], Step [13000/591753], Loss: 3.1574, Perplexity: 23.5102\n",
      "Epoch [1/1], Step [13100/591753], Loss: 2.8918, Perplexity: 18.02595\n",
      "Epoch [1/1], Step [13200/591753], Loss: 2.1654, Perplexity: 8.718290\n",
      "Epoch [1/1], Step [13300/591753], Loss: 1.2602, Perplexity: 3.526207\n",
      "Epoch [1/1], Step [13400/591753], Loss: 1.6968, Perplexity: 5.456201\n",
      "Epoch [1/1], Step [13500/591753], Loss: 1.3870, Perplexity: 4.00295\n",
      "Epoch [1/1], Step [13600/591753], Loss: 2.2356, Perplexity: 9.351981\n",
      "Epoch [1/1], Step [13700/591753], Loss: 2.2617, Perplexity: 9.59923\n",
      "Epoch [1/1], Step [13800/591753], Loss: 1.4313, Perplexity: 4.184064\n",
      "Epoch [1/1], Step [13900/591753], Loss: 2.0361, Perplexity: 7.660948\n",
      "Epoch [1/1], Step [14000/591753], Loss: 2.0246, Perplexity: 7.573038\n",
      "Epoch [1/1], Step [14100/591753], Loss: 2.9287, Perplexity: 18.70337\n",
      "Epoch [1/1], Step [14200/591753], Loss: 1.7923, Perplexity: 6.00335\n",
      "Epoch [1/1], Step [14300/591753], Loss: 1.9157, Perplexity: 6.792076\n",
      "Epoch [1/1], Step [14400/591753], Loss: 4.1702, Perplexity: 64.72914\n",
      "Epoch [1/1], Step [14500/591753], Loss: 1.1366, Perplexity: 3.11629\n",
      "Epoch [1/1], Step [14600/591753], Loss: 5.2330, Perplexity: 187.3571\n",
      "Epoch [1/1], Step [14700/591753], Loss: 2.9373, Perplexity: 18.86400\n",
      "Epoch [1/1], Step [14800/591753], Loss: 2.7811, Perplexity: 16.13641\n",
      "Epoch [1/1], Step [14900/591753], Loss: 2.2830, Perplexity: 9.805671\n",
      "Epoch [1/1], Step [15000/591753], Loss: 2.2044, Perplexity: 9.064970\n",
      "Epoch [1/1], Step [15100/591753], Loss: 3.6700, Perplexity: 39.25169\n",
      "Epoch [1/1], Step [15200/591753], Loss: 2.5417, Perplexity: 12.7012\n",
      "Epoch [1/1], Step [15300/591753], Loss: 2.0665, Perplexity: 7.897392\n",
      "Epoch [1/1], Step [15400/591753], Loss: 2.7523, Perplexity: 15.67883\n",
      "Epoch [1/1], Step [15500/591753], Loss: 2.6533, Perplexity: 14.20107\n",
      "Epoch [1/1], Step [15600/591753], Loss: 3.4918, Perplexity: 32.84623\n",
      "Epoch [1/1], Step [15700/591753], Loss: 3.8526, Perplexity: 47.11633\n",
      "Epoch [1/1], Step [15800/591753], Loss: 2.5799, Perplexity: 13.19588\n",
      "Epoch [1/1], Step [15900/591753], Loss: 2.2743, Perplexity: 9.72125\n",
      "Epoch [1/1], Step [16000/591753], Loss: 2.4693, Perplexity: 11.8143\n",
      "Epoch [1/1], Step [16100/591753], Loss: 1.8878, Perplexity: 6.604549\n",
      "Epoch [1/1], Step [16200/591753], Loss: 1.9705, Perplexity: 7.174478\n",
      "Epoch [1/1], Step [16300/591753], Loss: 2.6254, Perplexity: 13.81060\n",
      "Epoch [1/1], Step [16400/591753], Loss: 1.9988, Perplexity: 7.38023\n",
      "Epoch [1/1], Step [16500/591753], Loss: 3.1214, Perplexity: 22.67825\n",
      "Epoch [1/1], Step [16600/591753], Loss: 1.2565, Perplexity: 3.51291\n",
      "Epoch [1/1], Step [16700/591753], Loss: 2.8507, Perplexity: 17.30008\n",
      "Epoch [1/1], Step [16800/591753], Loss: 2.2752, Perplexity: 9.730389\n",
      "Epoch [1/1], Step [16900/591753], Loss: 1.5928, Perplexity: 4.917745\n",
      "Epoch [1/1], Step [17000/591753], Loss: 1.7504, Perplexity: 5.75690\n",
      "Epoch [1/1], Step [17100/591753], Loss: 1.6918, Perplexity: 5.429326\n",
      "Epoch [1/1], Step [17200/591753], Loss: 2.3416, Perplexity: 10.39780\n",
      "Epoch [1/1], Step [17300/591753], Loss: 1.5731, Perplexity: 4.82158\n",
      "Epoch [1/1], Step [17400/591753], Loss: 2.9878, Perplexity: 19.84199\n",
      "Epoch [1/1], Step [17500/591753], Loss: 2.2155, Perplexity: 9.166098\n",
      "Epoch [1/1], Step [17600/591753], Loss: 2.4315, Perplexity: 11.3759\n",
      "Epoch [1/1], Step [17700/591753], Loss: 4.2611, Perplexity: 70.88649\n",
      "Epoch [1/1], Step [17800/591753], Loss: 3.2838, Perplexity: 26.67562\n",
      "Epoch [1/1], Step [17900/591753], Loss: 3.3773, Perplexity: 29.29279\n",
      "Epoch [1/1], Step [18000/591753], Loss: 2.9938, Perplexity: 19.96109\n",
      "Epoch [1/1], Step [18100/591753], Loss: 1.3137, Perplexity: 3.719817\n",
      "Epoch [1/1], Step [18200/591753], Loss: 2.2015, Perplexity: 9.038774\n",
      "Epoch [1/1], Step [18300/591753], Loss: 3.1918, Perplexity: 24.3325\n",
      "Epoch [1/1], Step [18400/591753], Loss: 2.0019, Perplexity: 7.403055\n",
      "Epoch [1/1], Step [18500/591753], Loss: 5.2134, Perplexity: 183.7202\n",
      "Epoch [1/1], Step [18600/591753], Loss: 2.4071, Perplexity: 11.1014\n",
      "Epoch [1/1], Step [18700/591753], Loss: 3.6978, Perplexity: 40.35742\n",
      "Epoch [1/1], Step [18800/591753], Loss: 2.7955, Perplexity: 16.37111\n",
      "Epoch [1/1], Step [18900/591753], Loss: 2.3663, Perplexity: 10.65783\n",
      "Epoch [1/1], Step [19000/591753], Loss: 2.1094, Perplexity: 8.242933\n",
      "Epoch [1/1], Step [19100/591753], Loss: 1.0046, Perplexity: 2.730893\n",
      "Epoch [1/1], Step [19200/591753], Loss: 2.7434, Perplexity: 15.5393\n",
      "Epoch [1/1], Step [19300/591753], Loss: 1.9352, Perplexity: 6.92561\n",
      "Epoch [1/1], Step [19400/591753], Loss: 1.3181, Perplexity: 3.736505\n",
      "Epoch [1/1], Step [19500/591753], Loss: 1.9063, Perplexity: 6.728421\n",
      "Epoch [1/1], Step [19600/591753], Loss: 1.8706, Perplexity: 6.492440\n",
      "Epoch [1/1], Step [19700/591753], Loss: 3.5007, Perplexity: 33.1378\n",
      "Epoch [1/1], Step [19800/591753], Loss: 2.1395, Perplexity: 8.495430\n",
      "Epoch [1/1], Step [19900/591753], Loss: 3.6907, Perplexity: 40.07372\n",
      "Epoch [1/1], Step [20000/591753], Loss: 1.9052, Perplexity: 6.72055\n",
      "Epoch [1/1], Step [20100/591753], Loss: 1.6534, Perplexity: 5.22457\n",
      "Epoch [1/1], Step [20200/591753], Loss: 2.2534, Perplexity: 9.52049\n",
      "Epoch [1/1], Step [20300/591753], Loss: 2.1111, Perplexity: 8.25756\n",
      "Epoch [1/1], Step [20400/591753], Loss: 2.3310, Perplexity: 10.2882\n",
      "Epoch [1/1], Step [20500/591753], Loss: 3.8622, Perplexity: 47.57191\n",
      "Epoch [1/1], Step [20600/591753], Loss: 3.1758, Perplexity: 23.94706\n",
      "Epoch [1/1], Step [20700/591753], Loss: 2.6773, Perplexity: 14.5453\n",
      "Epoch [1/1], Step [20800/591753], Loss: 4.2266, Perplexity: 68.48673\n",
      "Epoch [1/1], Step [20900/591753], Loss: 2.8424, Perplexity: 17.15734\n",
      "Epoch [1/1], Step [21000/591753], Loss: 1.5194, Perplexity: 4.569369\n",
      "Epoch [1/1], Step [21100/591753], Loss: 2.3927, Perplexity: 10.94273\n",
      "Epoch [1/1], Step [21200/591753], Loss: 1.3534, Perplexity: 3.870505\n",
      "Epoch [1/1], Step [21300/591753], Loss: 1.8251, Perplexity: 6.203365\n",
      "Epoch [1/1], Step [21400/591753], Loss: 4.1831, Perplexity: 65.57077\n",
      "Epoch [1/1], Step [21500/591753], Loss: 1.7765, Perplexity: 5.908968\n",
      "Epoch [1/1], Step [21600/591753], Loss: 2.5058, Perplexity: 12.25305\n",
      "Epoch [1/1], Step [21700/591753], Loss: 2.0173, Perplexity: 7.518101\n",
      "Epoch [1/1], Step [21800/591753], Loss: 2.4941, Perplexity: 12.1106\n",
      "Epoch [1/1], Step [21900/591753], Loss: 4.0583, Perplexity: 57.87303\n",
      "Epoch [1/1], Step [22000/591753], Loss: 2.8505, Perplexity: 17.29620\n",
      "Epoch [1/1], Step [22100/591753], Loss: 1.2913, Perplexity: 3.63730\n",
      "Epoch [1/1], Step [22200/591753], Loss: 2.2341, Perplexity: 9.338032\n",
      "Epoch [1/1], Step [22300/591753], Loss: 1.7562, Perplexity: 5.790605\n",
      "Epoch [1/1], Step [22400/591753], Loss: 2.6891, Perplexity: 14.7185\n",
      "Epoch [1/1], Step [22500/591753], Loss: 2.3296, Perplexity: 10.27406\n",
      "Epoch [1/1], Step [22600/591753], Loss: 3.2327, Perplexity: 25.34877\n",
      "Epoch [1/1], Step [22700/591753], Loss: 2.0458, Perplexity: 7.735251\n",
      "Epoch [1/1], Step [22800/591753], Loss: 2.3424, Perplexity: 10.40623\n",
      "Epoch [1/1], Step [22900/591753], Loss: 1.4276, Perplexity: 4.16883\n",
      "Epoch [1/1], Step [23000/591753], Loss: 3.9413, Perplexity: 51.4834\n",
      "Epoch [1/1], Step [23100/591753], Loss: 2.7410, Perplexity: 15.50266\n",
      "Epoch [1/1], Step [23200/591753], Loss: 2.7905, Perplexity: 16.2890\n",
      "Epoch [1/1], Step [23300/591753], Loss: 3.8240, Perplexity: 45.78833\n",
      "Epoch [1/1], Step [23400/591753], Loss: 1.0006, Perplexity: 2.719977\n",
      "Epoch [1/1], Step [23500/591753], Loss: 2.9582, Perplexity: 19.26369\n",
      "Epoch [1/1], Step [23600/591753], Loss: 1.8390, Perplexity: 6.290355\n",
      "Epoch [1/1], Step [23700/591753], Loss: 3.6016, Perplexity: 36.65670\n",
      "Epoch [1/1], Step [23800/591753], Loss: 3.3558, Perplexity: 28.6677\n",
      "Epoch [1/1], Step [23900/591753], Loss: 2.5352, Perplexity: 12.61942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [24000/591753], Loss: 2.8664, Perplexity: 17.57407\n",
      "Epoch [1/1], Step [24100/591753], Loss: 1.4464, Perplexity: 4.247933\n",
      "Epoch [1/1], Step [24200/591753], Loss: 1.8586, Perplexity: 6.41455\n",
      "Epoch [1/1], Step [24300/591753], Loss: 1.1242, Perplexity: 3.077742\n",
      "Epoch [1/1], Step [24400/591753], Loss: 1.4048, Perplexity: 4.074687\n",
      "Epoch [1/1], Step [24500/591753], Loss: 2.6581, Perplexity: 14.26958\n",
      "Epoch [1/1], Step [24600/591753], Loss: 2.0053, Perplexity: 7.42836\n",
      "Epoch [1/1], Step [24700/591753], Loss: 1.7034, Perplexity: 5.492801\n",
      "Epoch [1/1], Step [24800/591753], Loss: 2.4156, Perplexity: 11.19635\n",
      "Epoch [1/1], Step [24900/591753], Loss: 2.3883, Perplexity: 10.89442\n",
      "Epoch [1/1], Step [25000/591753], Loss: 1.5272, Perplexity: 4.605127\n",
      "Epoch [1/1], Step [25100/591753], Loss: 2.1143, Perplexity: 8.283530\n",
      "Epoch [1/1], Step [25200/591753], Loss: 1.4463, Perplexity: 4.247380\n",
      "Epoch [1/1], Step [25300/591753], Loss: 2.9266, Perplexity: 18.66320\n",
      "Epoch [1/1], Step [25400/591753], Loss: 1.6205, Perplexity: 5.055587\n",
      "Epoch [1/1], Step [25500/591753], Loss: 2.3597, Perplexity: 10.58725\n",
      "Epoch [1/1], Step [25600/591753], Loss: 2.3844, Perplexity: 10.85241\n",
      "Epoch [1/1], Step [25700/591753], Loss: 1.5570, Perplexity: 4.744850\n",
      "Epoch [1/1], Step [25800/591753], Loss: 2.8616, Perplexity: 17.48992\n",
      "Epoch [1/1], Step [25900/591753], Loss: 2.7686, Perplexity: 15.93636\n",
      "Epoch [1/1], Step [26000/591753], Loss: 1.1303, Perplexity: 3.096614\n",
      "Epoch [1/1], Step [26100/591753], Loss: 1.7512, Perplexity: 5.761881\n",
      "Epoch [1/1], Step [26200/591753], Loss: 2.1623, Perplexity: 8.690847\n",
      "Epoch [1/1], Step [26300/591753], Loss: 1.4440, Perplexity: 4.237834\n",
      "Epoch [1/1], Step [26400/591753], Loss: 1.8290, Perplexity: 6.227441\n",
      "Epoch [1/1], Step [26500/591753], Loss: 2.3645, Perplexity: 10.63831\n",
      "Epoch [1/1], Step [26600/591753], Loss: 2.9310, Perplexity: 18.74634\n",
      "Epoch [1/1], Step [26700/591753], Loss: 2.2382, Perplexity: 9.376653\n",
      "Epoch [1/1], Step [26800/591753], Loss: 3.0459, Perplexity: 21.0288\n",
      "Epoch [1/1], Step [26900/591753], Loss: 1.8872, Perplexity: 6.600902\n",
      "Epoch [1/1], Step [27000/591753], Loss: 3.8029, Perplexity: 44.83116\n",
      "Epoch [1/1], Step [27100/591753], Loss: 0.8002, Perplexity: 2.226143\n",
      "Epoch [1/1], Step [27200/591753], Loss: 1.9743, Perplexity: 7.201712\n",
      "Epoch [1/1], Step [27300/591753], Loss: 1.5269, Perplexity: 4.604105\n",
      "Epoch [1/1], Step [27400/591753], Loss: 2.9382, Perplexity: 18.88249\n",
      "Epoch [1/1], Step [27500/591753], Loss: 2.0685, Perplexity: 7.912892\n",
      "Epoch [1/1], Step [27600/591753], Loss: 1.0266, Perplexity: 2.79154\n",
      "Epoch [1/1], Step [27700/591753], Loss: 3.3699, Perplexity: 29.0768\n",
      "Epoch [1/1], Step [27800/591753], Loss: 3.2284, Perplexity: 25.2392\n",
      "Epoch [1/1], Step [27900/591753], Loss: 2.4930, Perplexity: 12.0977\n",
      "Epoch [1/1], Step [28000/591753], Loss: 2.8145, Perplexity: 16.68495\n",
      "Epoch [1/1], Step [28100/591753], Loss: 1.9747, Perplexity: 7.204250\n",
      "Epoch [1/1], Step [28200/591753], Loss: 1.9359, Perplexity: 6.930663\n",
      "Epoch [1/1], Step [28300/591753], Loss: 3.7586, Perplexity: 42.88720\n",
      "Epoch [1/1], Step [28400/591753], Loss: 2.3499, Perplexity: 10.4842\n",
      "Epoch [1/1], Step [28500/591753], Loss: 2.3519, Perplexity: 10.5059\n",
      "Epoch [1/1], Step [28600/591753], Loss: 1.4623, Perplexity: 4.315986\n",
      "Epoch [1/1], Step [28700/591753], Loss: 2.8415, Perplexity: 17.1407\n",
      "Epoch [1/1], Step [28800/591753], Loss: 2.3539, Perplexity: 10.52683\n",
      "Epoch [1/1], Step [28900/591753], Loss: 1.1965, Perplexity: 3.308407\n",
      "Epoch [1/1], Step [29000/591753], Loss: 1.6183, Perplexity: 5.04449\n",
      "Epoch [1/1], Step [29100/591753], Loss: 2.4038, Perplexity: 11.0646\n",
      "Epoch [1/1], Step [29200/591753], Loss: 1.3947, Perplexity: 4.033967\n",
      "Epoch [1/1], Step [29300/591753], Loss: 2.4656, Perplexity: 11.7706\n",
      "Epoch [1/1], Step [29400/591753], Loss: 2.8134, Perplexity: 16.66717\n",
      "Epoch [1/1], Step [29500/591753], Loss: 4.1396, Perplexity: 62.77775\n",
      "Epoch [1/1], Step [29600/591753], Loss: 1.4848, Perplexity: 4.414106\n",
      "Epoch [1/1], Step [29700/591753], Loss: 1.9679, Perplexity: 7.15597\n",
      "Epoch [1/1], Step [29800/591753], Loss: 2.6516, Perplexity: 14.1768\n",
      "Epoch [1/1], Step [29900/591753], Loss: 2.0331, Perplexity: 7.637794\n",
      "Epoch [1/1], Step [30000/591753], Loss: 3.4925, Perplexity: 32.86863\n",
      "Epoch [1/1], Step [30100/591753], Loss: 2.2743, Perplexity: 9.721342\n",
      "Epoch [1/1], Step [30200/591753], Loss: 3.9027, Perplexity: 49.53361\n",
      "Epoch [1/1], Step [30300/591753], Loss: 2.7781, Perplexity: 16.08836\n",
      "Epoch [1/1], Step [30400/591753], Loss: 2.7762, Perplexity: 16.05863\n",
      "Epoch [1/1], Step [30500/591753], Loss: 2.2281, Perplexity: 9.281812\n",
      "Epoch [1/1], Step [30600/591753], Loss: 2.3188, Perplexity: 10.16342\n",
      "Epoch [1/1], Step [30700/591753], Loss: 2.3433, Perplexity: 10.41532\n",
      "Epoch [1/1], Step [30800/591753], Loss: 3.1894, Perplexity: 24.27410\n",
      "Epoch [1/1], Step [30900/591753], Loss: 2.6565, Perplexity: 14.24669\n",
      "Epoch [1/1], Step [31000/591753], Loss: 2.1343, Perplexity: 8.451387\n",
      "Epoch [1/1], Step [31100/591753], Loss: 2.8325, Perplexity: 16.98847\n",
      "Epoch [1/1], Step [31200/591753], Loss: 2.2987, Perplexity: 9.960908\n",
      "Epoch [1/1], Step [31300/591753], Loss: 2.0916, Perplexity: 8.097878\n",
      "Epoch [1/1], Step [31400/591753], Loss: 2.3802, Perplexity: 10.80710\n",
      "Epoch [1/1], Step [31500/591753], Loss: 4.1670, Perplexity: 64.52033\n",
      "Epoch [1/1], Step [31600/591753], Loss: 2.6715, Perplexity: 14.46203\n",
      "Epoch [1/1], Step [31700/591753], Loss: 2.4015, Perplexity: 11.03963\n",
      "Epoch [1/1], Step [31800/591753], Loss: 2.8638, Perplexity: 17.5287\n",
      "Epoch [1/1], Step [31900/591753], Loss: 1.5185, Perplexity: 4.56558\n",
      "Epoch [1/1], Step [32000/591753], Loss: 2.2508, Perplexity: 9.49588\n",
      "Epoch [1/1], Step [32100/591753], Loss: 3.7599, Perplexity: 42.94603\n",
      "Epoch [1/1], Step [32200/591753], Loss: 2.4860, Perplexity: 12.01371\n",
      "Epoch [1/1], Step [32300/591753], Loss: 2.4722, Perplexity: 11.8479\n",
      "Epoch [1/1], Step [32400/591753], Loss: 2.8228, Perplexity: 16.82350\n",
      "Epoch [1/1], Step [32500/591753], Loss: 2.2408, Perplexity: 9.40067\n",
      "Epoch [1/1], Step [32600/591753], Loss: 3.1494, Perplexity: 23.32231\n",
      "Epoch [1/1], Step [32700/591753], Loss: 1.8290, Perplexity: 6.22741\n",
      "Epoch [1/1], Step [32800/591753], Loss: 3.6035, Perplexity: 36.7259\n",
      "Epoch [1/1], Step [32900/591753], Loss: 2.3452, Perplexity: 10.4358\n",
      "Epoch [1/1], Step [33000/591753], Loss: 1.3650, Perplexity: 3.915970\n",
      "Epoch [1/1], Step [33100/591753], Loss: 1.9436, Perplexity: 6.983787\n",
      "Epoch [1/1], Step [33200/591753], Loss: 2.5602, Perplexity: 12.93844\n",
      "Epoch [1/1], Step [33300/591753], Loss: 3.9646, Perplexity: 52.6972\n",
      "Epoch [1/1], Step [33400/591753], Loss: 1.1700, Perplexity: 3.221977\n",
      "Epoch [1/1], Step [33500/591753], Loss: 1.1653, Perplexity: 3.20705\n",
      "Epoch [1/1], Step [33600/591753], Loss: 3.6163, Perplexity: 37.20131\n",
      "Epoch [1/1], Step [33700/591753], Loss: 1.0851, Perplexity: 2.959790\n",
      "Epoch [1/1], Step [33800/591753], Loss: 3.0311, Perplexity: 20.7198\n",
      "Epoch [1/1], Step [33900/591753], Loss: 2.0227, Perplexity: 7.55905\n",
      "Epoch [1/1], Step [34000/591753], Loss: 2.1894, Perplexity: 8.930104\n",
      "Epoch [1/1], Step [34100/591753], Loss: 2.5242, Perplexity: 12.48159\n",
      "Epoch [1/1], Step [34200/591753], Loss: 2.1383, Perplexity: 8.485230\n",
      "Epoch [1/1], Step [34300/591753], Loss: 3.4890, Perplexity: 32.75282\n",
      "Epoch [1/1], Step [34400/591753], Loss: 2.6337, Perplexity: 13.9250\n",
      "Epoch [1/1], Step [34500/591753], Loss: 6.2748, Perplexity: 531.0137\n",
      "Epoch [1/1], Step [34600/591753], Loss: 2.1193, Perplexity: 8.325446\n",
      "Epoch [1/1], Step [34700/591753], Loss: 2.1592, Perplexity: 8.66430\n",
      "Epoch [1/1], Step [34800/591753], Loss: 2.9100, Perplexity: 18.35767\n",
      "Epoch [1/1], Step [34900/591753], Loss: 2.3533, Perplexity: 10.52041\n",
      "Epoch [1/1], Step [35000/591753], Loss: 2.6747, Perplexity: 14.50860\n",
      "Epoch [1/1], Step [35100/591753], Loss: 1.1012, Perplexity: 3.007857\n",
      "Epoch [1/1], Step [35200/591753], Loss: 1.6230, Perplexity: 5.068493\n",
      "Epoch [1/1], Step [35300/591753], Loss: 2.1575, Perplexity: 8.649750\n",
      "Epoch [1/1], Step [35400/591753], Loss: 3.8976, Perplexity: 49.28249\n",
      "Epoch [1/1], Step [35500/591753], Loss: 1.2917, Perplexity: 3.639012\n",
      "Epoch [1/1], Step [35600/591753], Loss: 1.0956, Perplexity: 2.991176\n",
      "Epoch [1/1], Step [35700/591753], Loss: 1.3566, Perplexity: 3.883159\n",
      "Epoch [1/1], Step [35800/591753], Loss: 2.0683, Perplexity: 7.91149\n",
      "Epoch [1/1], Step [35900/591753], Loss: 2.5493, Perplexity: 12.79835\n",
      "Epoch [1/1], Step [36000/591753], Loss: 1.5941, Perplexity: 4.924014\n",
      "Epoch [1/1], Step [36100/591753], Loss: 2.7939, Perplexity: 16.34437\n",
      "Epoch [1/1], Step [36200/591753], Loss: 2.8225, Perplexity: 16.8192\n",
      "Epoch [1/1], Step [36300/591753], Loss: 2.3561, Perplexity: 10.54947\n",
      "Epoch [1/1], Step [36400/591753], Loss: 3.3392, Perplexity: 28.19627\n",
      "Epoch [1/1], Step [36500/591753], Loss: 2.6228, Perplexity: 13.77404\n",
      "Epoch [1/1], Step [36600/591753], Loss: 2.5323, Perplexity: 12.58185\n",
      "Epoch [1/1], Step [36700/591753], Loss: 2.7376, Perplexity: 15.4506\n",
      "Epoch [1/1], Step [36800/591753], Loss: 2.6026, Perplexity: 13.49872\n",
      "Epoch [1/1], Step [36900/591753], Loss: 2.0090, Perplexity: 7.45595\n",
      "Epoch [1/1], Step [37000/591753], Loss: 1.5755, Perplexity: 4.833086\n",
      "Epoch [1/1], Step [37100/591753], Loss: 2.7112, Perplexity: 15.04694\n",
      "Epoch [1/1], Step [37200/591753], Loss: 3.2008, Perplexity: 24.55226\n",
      "Epoch [1/1], Step [37300/591753], Loss: 1.6207, Perplexity: 5.05666\n",
      "Epoch [1/1], Step [37400/591753], Loss: 1.9476, Perplexity: 7.011746\n",
      "Epoch [1/1], Step [37500/591753], Loss: 2.7710, Perplexity: 15.97445\n",
      "Epoch [1/1], Step [37600/591753], Loss: 2.9925, Perplexity: 19.93471\n",
      "Epoch [1/1], Step [37700/591753], Loss: 2.0833, Perplexity: 8.030956\n",
      "Epoch [1/1], Step [37800/591753], Loss: 2.3722, Perplexity: 10.7212\n",
      "Epoch [1/1], Step [37900/591753], Loss: 1.7838, Perplexity: 5.952366\n",
      "Epoch [1/1], Step [38000/591753], Loss: 2.4330, Perplexity: 11.39292\n",
      "Epoch [1/1], Step [38100/591753], Loss: 1.9955, Perplexity: 7.356124\n",
      "Epoch [1/1], Step [38200/591753], Loss: 1.0704, Perplexity: 2.916494\n",
      "Epoch [1/1], Step [38300/591753], Loss: 4.0274, Perplexity: 56.11378\n",
      "Epoch [1/1], Step [38400/591753], Loss: 2.9788, Perplexity: 19.66492\n",
      "Epoch [1/1], Step [38500/591753], Loss: 1.6349, Perplexity: 5.129155\n",
      "Epoch [1/1], Step [38600/591753], Loss: 1.0857, Perplexity: 2.961657\n",
      "Epoch [1/1], Step [38700/591753], Loss: 2.2065, Perplexity: 9.08432\n",
      "Epoch [1/1], Step [38800/591753], Loss: 5.4551, Perplexity: 233.9368\n",
      "Epoch [1/1], Step [38900/591753], Loss: 2.0758, Perplexity: 7.97051\n",
      "Epoch [1/1], Step [39000/591753], Loss: 1.4808, Perplexity: 4.396686\n",
      "Epoch [1/1], Step [39100/591753], Loss: 0.8245, Perplexity: 2.28091\n",
      "Epoch [1/1], Step [39200/591753], Loss: 2.4947, Perplexity: 12.1180\n",
      "Epoch [1/1], Step [39300/591753], Loss: 1.3365, Perplexity: 3.805976\n",
      "Epoch [1/1], Step [39400/591753], Loss: 1.7163, Perplexity: 5.563895\n",
      "Epoch [1/1], Step [39500/591753], Loss: 2.0511, Perplexity: 7.776419\n",
      "Epoch [1/1], Step [39600/591753], Loss: 2.5826, Perplexity: 13.23165\n",
      "Epoch [1/1], Step [39700/591753], Loss: 2.3519, Perplexity: 10.50560\n",
      "Epoch [1/1], Step [39800/591753], Loss: 1.6849, Perplexity: 5.392081\n",
      "Epoch [1/1], Step [39900/591753], Loss: 2.7757, Perplexity: 16.04915\n",
      "Epoch [1/1], Step [40000/591753], Loss: 1.3625, Perplexity: 3.905935\n",
      "Epoch [1/1], Step [40100/591753], Loss: 1.0088, Perplexity: 2.742428\n",
      "Epoch [1/1], Step [40200/591753], Loss: 3.8680, Perplexity: 47.84516\n",
      "Epoch [1/1], Step [40300/591753], Loss: 1.2721, Perplexity: 3.568472\n",
      "Epoch [1/1], Step [40400/591753], Loss: 1.3296, Perplexity: 3.779517\n",
      "Epoch [1/1], Step [40500/591753], Loss: 3.8447, Perplexity: 46.7469\n",
      "Epoch [1/1], Step [40600/591753], Loss: 2.7684, Perplexity: 15.93334\n",
      "Epoch [1/1], Step [40700/591753], Loss: 1.0553, Perplexity: 2.872854\n",
      "Epoch [1/1], Step [40800/591753], Loss: 3.3190, Perplexity: 27.6335\n",
      "Epoch [1/1], Step [40900/591753], Loss: 1.6490, Perplexity: 5.201884\n",
      "Epoch [1/1], Step [41000/591753], Loss: 2.4972, Perplexity: 12.14809\n",
      "Epoch [1/1], Step [41100/591753], Loss: 2.6892, Perplexity: 14.71956\n",
      "Epoch [1/1], Step [41200/591753], Loss: 2.2109, Perplexity: 9.124161\n",
      "Epoch [1/1], Step [41300/591753], Loss: 3.0146, Perplexity: 20.38156\n",
      "Epoch [1/1], Step [41400/591753], Loss: 1.5976, Perplexity: 4.941417\n",
      "Epoch [1/1], Step [41500/591753], Loss: 1.2129, Perplexity: 3.363413\n",
      "Epoch [1/1], Step [41600/591753], Loss: 2.1207, Perplexity: 8.336983\n",
      "Epoch [1/1], Step [41700/591753], Loss: 3.7669, Perplexity: 43.24418\n",
      "Epoch [1/1], Step [41800/591753], Loss: 4.5162, Perplexity: 91.4899\n",
      "Epoch [1/1], Step [41900/591753], Loss: 4.0589, Perplexity: 57.91270\n",
      "Epoch [1/1], Step [42000/591753], Loss: 2.4668, Perplexity: 11.78475\n",
      "Epoch [1/1], Step [42100/591753], Loss: 1.9022, Perplexity: 6.700576\n",
      "Epoch [1/1], Step [42200/591753], Loss: 2.4151, Perplexity: 11.19053\n",
      "Epoch [1/1], Step [42300/591753], Loss: 2.1187, Perplexity: 8.320215\n",
      "Epoch [1/1], Step [42400/591753], Loss: 2.4632, Perplexity: 11.74222\n",
      "Epoch [1/1], Step [42500/591753], Loss: 3.6091, Perplexity: 36.93231\n",
      "Epoch [1/1], Step [42600/591753], Loss: 1.6839, Perplexity: 5.386316\n",
      "Epoch [1/1], Step [42700/591753], Loss: 1.5331, Perplexity: 4.632445\n",
      "Epoch [1/1], Step [42800/591753], Loss: 2.4847, Perplexity: 11.9972\n",
      "Epoch [1/1], Step [42900/591753], Loss: 2.9033, Perplexity: 18.2348\n",
      "Epoch [1/1], Step [43000/591753], Loss: 3.0850, Perplexity: 21.86786\n",
      "Epoch [1/1], Step [43100/591753], Loss: 1.9371, Perplexity: 6.93866\n",
      "Epoch [1/1], Step [43200/591753], Loss: 2.5956, Perplexity: 13.4052\n",
      "Epoch [1/1], Step [43300/591753], Loss: 3.6428, Perplexity: 38.1984\n",
      "Epoch [1/1], Step [43400/591753], Loss: 3.5832, Perplexity: 35.98875\n",
      "Epoch [1/1], Step [43500/591753], Loss: 0.7801, Perplexity: 2.181862\n",
      "Epoch [1/1], Step [43600/591753], Loss: 2.5059, Perplexity: 12.2550\n",
      "Epoch [1/1], Step [43700/591753], Loss: 2.5047, Perplexity: 12.24006\n",
      "Epoch [1/1], Step [43800/591753], Loss: 2.2027, Perplexity: 9.049947\n",
      "Epoch [1/1], Step [43900/591753], Loss: 0.8625, Perplexity: 2.36905\n",
      "Epoch [1/1], Step [44000/591753], Loss: 2.4513, Perplexity: 11.6039\n",
      "Epoch [1/1], Step [44100/591753], Loss: 2.7080, Perplexity: 14.99968\n",
      "Epoch [1/1], Step [44200/591753], Loss: 3.4479, Perplexity: 31.43319\n",
      "Epoch [1/1], Step [44300/591753], Loss: 3.4377, Perplexity: 31.1149\n",
      "Epoch [1/1], Step [44400/591753], Loss: 2.9018, Perplexity: 18.20668\n",
      "Epoch [1/1], Step [44500/591753], Loss: 1.7800, Perplexity: 5.929941\n",
      "Epoch [1/1], Step [44600/591753], Loss: 1.1074, Perplexity: 3.026513\n",
      "Epoch [1/1], Step [44700/591753], Loss: 2.1979, Perplexity: 9.00572\n",
      "Epoch [1/1], Step [44800/591753], Loss: 2.4113, Perplexity: 11.1482\n",
      "Epoch [1/1], Step [44900/591753], Loss: 2.6146, Perplexity: 13.6619\n",
      "Epoch [1/1], Step [45000/591753], Loss: 2.9477, Perplexity: 19.06307\n",
      "Epoch [1/1], Step [45100/591753], Loss: 2.6857, Perplexity: 14.66887\n",
      "Epoch [1/1], Step [45200/591753], Loss: 3.6128, Perplexity: 37.0716\n",
      "Epoch [1/1], Step [45300/591753], Loss: 1.9944, Perplexity: 7.34803\n",
      "Epoch [1/1], Step [45400/591753], Loss: 2.2856, Perplexity: 9.831500\n",
      "Epoch [1/1], Step [45500/591753], Loss: 1.8191, Perplexity: 6.166197\n",
      "Epoch [1/1], Step [45600/591753], Loss: 3.5825, Perplexity: 35.96455\n",
      "Epoch [1/1], Step [45700/591753], Loss: 3.8952, Perplexity: 49.1651\n",
      "Epoch [1/1], Step [45800/591753], Loss: 0.7596, Perplexity: 2.137510\n",
      "Epoch [1/1], Step [45900/591753], Loss: 2.2915, Perplexity: 9.89028\n",
      "Epoch [1/1], Step [46000/591753], Loss: 2.9862, Perplexity: 19.8097\n",
      "Epoch [1/1], Step [46100/591753], Loss: 2.2378, Perplexity: 9.372674\n",
      "Epoch [1/1], Step [46200/591753], Loss: 2.0516, Perplexity: 7.780381\n",
      "Epoch [1/1], Step [46300/591753], Loss: 2.3171, Perplexity: 10.14601\n",
      "Epoch [1/1], Step [46400/591753], Loss: 1.9879, Perplexity: 7.300185\n",
      "Epoch [1/1], Step [46500/591753], Loss: 2.2252, Perplexity: 9.255088\n",
      "Epoch [1/1], Step [46600/591753], Loss: 2.2055, Perplexity: 9.074979\n",
      "Epoch [1/1], Step [46700/591753], Loss: 1.7584, Perplexity: 5.802974\n",
      "Epoch [1/1], Step [46800/591753], Loss: 3.4267, Perplexity: 30.77468\n",
      "Epoch [1/1], Step [46900/591753], Loss: 2.8287, Perplexity: 16.9236\n",
      "Epoch [1/1], Step [47000/591753], Loss: 2.3341, Perplexity: 10.3206\n",
      "Epoch [1/1], Step [47100/591753], Loss: 4.7256, Perplexity: 112.8024\n",
      "Epoch [1/1], Step [47200/591753], Loss: 0.9868, Perplexity: 2.68275\n",
      "Epoch [1/1], Step [47300/591753], Loss: 2.2504, Perplexity: 9.49158\n",
      "Epoch [1/1], Step [47400/591753], Loss: 2.1146, Perplexity: 8.286441\n",
      "Epoch [1/1], Step [47500/591753], Loss: 1.6405, Perplexity: 5.15808\n",
      "Epoch [1/1], Step [47600/591753], Loss: 2.5478, Perplexity: 12.77897\n",
      "Epoch [1/1], Step [47700/591753], Loss: 1.6303, Perplexity: 5.105513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [47800/591753], Loss: 2.0782, Perplexity: 7.98980\n",
      "Epoch [1/1], Step [47900/591753], Loss: 3.0011, Perplexity: 20.1067\n",
      "Epoch [1/1], Step [48000/591753], Loss: 3.3700, Perplexity: 29.07934\n",
      "Epoch [1/1], Step [48100/591753], Loss: 1.5471, Perplexity: 4.697801\n",
      "Epoch [1/1], Step [48200/591753], Loss: 2.1599, Perplexity: 8.669925\n",
      "Epoch [1/1], Step [48300/591753], Loss: 2.2377, Perplexity: 9.371766\n",
      "Epoch [1/1], Step [48400/591753], Loss: 3.6317, Perplexity: 37.77649\n",
      "Epoch [1/1], Step [48500/591753], Loss: 2.9151, Perplexity: 18.4512\n",
      "Epoch [1/1], Step [48600/591753], Loss: 3.7601, Perplexity: 42.95476\n",
      "Epoch [1/1], Step [48700/591753], Loss: 1.9116, Perplexity: 6.763703\n",
      "Epoch [1/1], Step [48800/591753], Loss: 1.5535, Perplexity: 4.728175\n",
      "Epoch [1/1], Step [48900/591753], Loss: 1.8228, Perplexity: 6.18959\n",
      "Epoch [1/1], Step [49000/591753], Loss: 1.4459, Perplexity: 4.245817\n",
      "Epoch [1/1], Step [49100/591753], Loss: 3.6382, Perplexity: 38.02486\n",
      "Epoch [1/1], Step [49200/591753], Loss: 1.9315, Perplexity: 6.900113\n",
      "Epoch [1/1], Step [49300/591753], Loss: 1.5105, Perplexity: 4.529240\n",
      "Epoch [1/1], Step [49400/591753], Loss: 1.6024, Perplexity: 4.964742\n",
      "Epoch [1/1], Step [49500/591753], Loss: 1.6190, Perplexity: 5.048325\n",
      "Epoch [1/1], Step [49600/591753], Loss: 2.5949, Perplexity: 13.39520\n",
      "Epoch [1/1], Step [49700/591753], Loss: 1.7990, Perplexity: 6.043714\n",
      "Epoch [1/1], Step [49800/591753], Loss: 1.6731, Perplexity: 5.328525\n",
      "Epoch [1/1], Step [49900/591753], Loss: 2.9512, Perplexity: 19.12837\n",
      "Epoch [1/1], Step [50000/591753], Loss: 2.8240, Perplexity: 16.84405\n",
      "Epoch [1/1], Step [50100/591753], Loss: 2.0802, Perplexity: 8.006170\n",
      "Epoch [1/1], Step [50200/591753], Loss: 2.4600, Perplexity: 11.7048\n",
      "Epoch [1/1], Step [50300/591753], Loss: 2.6438, Perplexity: 14.06694\n",
      "Epoch [1/1], Step [50400/591753], Loss: 1.7546, Perplexity: 5.78148\n",
      "Epoch [1/1], Step [50500/591753], Loss: 2.3734, Perplexity: 10.73404\n",
      "Epoch [1/1], Step [50600/591753], Loss: 2.3167, Perplexity: 10.14241\n",
      "Epoch [1/1], Step [50700/591753], Loss: 2.7735, Perplexity: 16.01449\n",
      "Epoch [1/1], Step [50800/591753], Loss: 1.7403, Perplexity: 5.698850\n",
      "Epoch [1/1], Step [50900/591753], Loss: 1.1934, Perplexity: 3.29825\n",
      "Epoch [1/1], Step [51000/591753], Loss: 2.6874, Perplexity: 14.69389\n",
      "Epoch [1/1], Step [51100/591753], Loss: 4.9774, Perplexity: 145.0914\n",
      "Epoch [1/1], Step [51200/591753], Loss: 1.4582, Perplexity: 4.298305\n",
      "Epoch [1/1], Step [51300/591753], Loss: 1.4175, Perplexity: 4.126853\n",
      "Epoch [1/1], Step [51400/591753], Loss: 1.8105, Perplexity: 6.113785\n",
      "Epoch [1/1], Step [51500/591753], Loss: 4.9741, Perplexity: 144.6155\n",
      "Epoch [1/1], Step [51600/591753], Loss: 3.0689, Perplexity: 21.51789\n",
      "Epoch [1/1], Step [51700/591753], Loss: 2.2973, Perplexity: 9.947218\n",
      "Epoch [1/1], Step [51800/591753], Loss: 3.0962, Perplexity: 22.1145\n",
      "Epoch [1/1], Step [51900/591753], Loss: 2.5315, Perplexity: 12.57229\n",
      "Epoch [1/1], Step [52000/591753], Loss: 2.9187, Perplexity: 18.5170\n",
      "Epoch [1/1], Step [52100/591753], Loss: 1.9002, Perplexity: 6.687431\n",
      "Epoch [1/1], Step [52200/591753], Loss: 4.7697, Perplexity: 117.8830\n",
      "Epoch [1/1], Step [52300/591753], Loss: 1.4706, Perplexity: 4.352035\n",
      "Epoch [1/1], Step [52400/591753], Loss: 2.9063, Perplexity: 18.28835\n",
      "Epoch [1/1], Step [52500/591753], Loss: 1.0421, Perplexity: 2.83537\n",
      "Epoch [1/1], Step [52600/591753], Loss: 1.7667, Perplexity: 5.851820\n",
      "Epoch [1/1], Step [52700/591753], Loss: 1.1926, Perplexity: 3.295616\n",
      "Epoch [1/1], Step [52800/591753], Loss: 1.5783, Perplexity: 4.84695\n",
      "Epoch [1/1], Step [52900/591753], Loss: 2.4899, Perplexity: 12.05959\n",
      "Epoch [1/1], Step [53000/591753], Loss: 3.4904, Perplexity: 32.79790\n",
      "Epoch [1/1], Step [53100/591753], Loss: 2.4760, Perplexity: 11.89418\n",
      "Epoch [1/1], Step [53200/591753], Loss: 3.3880, Perplexity: 29.60777\n",
      "Epoch [1/1], Step [53300/591753], Loss: 1.4068, Perplexity: 4.083025\n",
      "Epoch [1/1], Step [53400/591753], Loss: 2.2318, Perplexity: 9.316926\n",
      "Epoch [1/1], Step [53500/591753], Loss: 1.8048, Perplexity: 6.078865\n",
      "Epoch [1/1], Step [53600/591753], Loss: 1.3922, Perplexity: 4.023995\n",
      "Epoch [1/1], Step [53700/591753], Loss: 3.0109, Perplexity: 20.3063\n",
      "Epoch [1/1], Step [53800/591753], Loss: 2.3497, Perplexity: 10.48288\n",
      "Epoch [1/1], Step [53900/591753], Loss: 1.9213, Perplexity: 6.829943\n",
      "Epoch [1/1], Step [54000/591753], Loss: 2.6446, Perplexity: 14.0772\n",
      "Epoch [1/1], Step [54100/591753], Loss: 3.2502, Perplexity: 25.7954\n",
      "Epoch [1/1], Step [54200/591753], Loss: 2.2450, Perplexity: 9.44015\n",
      "Epoch [1/1], Step [54300/591753], Loss: 3.2108, Perplexity: 24.79830\n",
      "Epoch [1/1], Step [54400/591753], Loss: 3.0538, Perplexity: 21.1951\n",
      "Epoch [1/1], Step [54500/591753], Loss: 2.6179, Perplexity: 13.70680\n",
      "Epoch [1/1], Step [54600/591753], Loss: 2.7020, Perplexity: 14.90908\n",
      "Epoch [1/1], Step [54700/591753], Loss: 1.6375, Perplexity: 5.142474\n",
      "Epoch [1/1], Step [54800/591753], Loss: 1.6987, Perplexity: 5.46680\n",
      "Epoch [1/1], Step [54900/591753], Loss: 3.7968, Perplexity: 44.55863\n",
      "Epoch [1/1], Step [55000/591753], Loss: 2.1571, Perplexity: 8.645681\n",
      "Epoch [1/1], Step [55100/591753], Loss: 1.4308, Perplexity: 4.182128\n",
      "Epoch [1/1], Step [55200/591753], Loss: 2.0612, Perplexity: 7.85550\n",
      "Epoch [1/1], Step [55300/591753], Loss: 3.4414, Perplexity: 31.23086\n",
      "Epoch [1/1], Step [55400/591753], Loss: 1.8181, Perplexity: 6.160246\n",
      "Epoch [1/1], Step [55500/591753], Loss: 2.9320, Perplexity: 18.76603\n",
      "Epoch [1/1], Step [55600/591753], Loss: 2.0966, Perplexity: 8.138620\n",
      "Epoch [1/1], Step [55700/591753], Loss: 3.7727, Perplexity: 43.49819\n",
      "Epoch [1/1], Step [55800/591753], Loss: 2.5232, Perplexity: 12.4685\n",
      "Epoch [1/1], Step [55900/591753], Loss: 2.2923, Perplexity: 9.898078\n",
      "Epoch [1/1], Step [56000/591753], Loss: 2.5804, Perplexity: 13.2024\n",
      "Epoch [1/1], Step [56100/591753], Loss: 1.4657, Perplexity: 4.330663\n",
      "Epoch [1/1], Step [56200/591753], Loss: 1.8191, Perplexity: 6.16667\n",
      "Epoch [1/1], Step [56300/591753], Loss: 2.3635, Perplexity: 10.6280\n",
      "Epoch [1/1], Step [56400/591753], Loss: 1.7318, Perplexity: 5.651001\n",
      "Epoch [1/1], Step [56500/591753], Loss: 2.1614, Perplexity: 8.68365\n",
      "Epoch [1/1], Step [56600/591753], Loss: 1.8291, Perplexity: 6.228416\n",
      "Epoch [1/1], Step [56700/591753], Loss: 2.7506, Perplexity: 15.65273\n",
      "Epoch [1/1], Step [56800/591753], Loss: 2.3004, Perplexity: 9.977814\n",
      "Epoch [1/1], Step [56900/591753], Loss: 2.5253, Perplexity: 12.49419\n",
      "Epoch [1/1], Step [57000/591753], Loss: 2.5454, Perplexity: 12.74884\n",
      "Epoch [1/1], Step [57100/591753], Loss: 1.0861, Perplexity: 2.962801\n",
      "Epoch [1/1], Step [57200/591753], Loss: 2.2722, Perplexity: 9.701001\n",
      "Epoch [1/1], Step [57300/591753], Loss: 1.8429, Perplexity: 6.314613\n",
      "Epoch [1/1], Step [57400/591753], Loss: 1.7441, Perplexity: 5.721074\n",
      "Epoch [1/1], Step [57500/591753], Loss: 3.9220, Perplexity: 50.5010\n",
      "Epoch [1/1], Step [57600/591753], Loss: 1.2479, Perplexity: 3.483174\n",
      "Epoch [1/1], Step [57700/591753], Loss: 2.0106, Perplexity: 7.468236\n",
      "Epoch [1/1], Step [57800/591753], Loss: 2.5072, Perplexity: 12.27101\n",
      "Epoch [1/1], Step [57900/591753], Loss: 2.5455, Perplexity: 12.74978\n",
      "Epoch [1/1], Step [58000/591753], Loss: 2.8626, Perplexity: 17.50630\n",
      "Epoch [1/1], Step [58100/591753], Loss: 1.4433, Perplexity: 4.234781\n",
      "Epoch [1/1], Step [58200/591753], Loss: 3.2290, Perplexity: 25.25514\n",
      "Epoch [1/1], Step [58300/591753], Loss: 3.6761, Perplexity: 39.49269\n",
      "Epoch [1/1], Step [58400/591753], Loss: 2.1198, Perplexity: 8.32964\n",
      "Epoch [1/1], Step [58500/591753], Loss: 2.5966, Perplexity: 13.41856\n",
      "Epoch [1/1], Step [58600/591753], Loss: 2.6437, Perplexity: 14.06502\n",
      "Epoch [1/1], Step [58700/591753], Loss: 2.3142, Perplexity: 10.11705\n",
      "Epoch [1/1], Step [58800/591753], Loss: 4.5227, Perplexity: 92.08641\n",
      "Epoch [1/1], Step [58900/591753], Loss: 2.6268, Perplexity: 13.8292\n",
      "Epoch [1/1], Step [59000/591753], Loss: 1.9586, Perplexity: 7.08953\n",
      "Epoch [1/1], Step [59100/591753], Loss: 4.7904, Perplexity: 120.3465\n",
      "Epoch [1/1], Step [59200/591753], Loss: 1.1907, Perplexity: 3.28929\n",
      "Epoch [1/1], Step [59300/591753], Loss: 3.5163, Perplexity: 33.66104\n",
      "Epoch [1/1], Step [59400/591753], Loss: 2.5066, Perplexity: 12.26340\n",
      "Epoch [1/1], Step [59500/591753], Loss: 2.6385, Perplexity: 13.99286\n",
      "Epoch [1/1], Step [59600/591753], Loss: 2.7586, Perplexity: 15.77781\n",
      "Epoch [1/1], Step [59700/591753], Loss: 1.7667, Perplexity: 5.851303\n",
      "Epoch [1/1], Step [59800/591753], Loss: 3.7351, Perplexity: 41.89374\n",
      "Epoch [1/1], Step [59900/591753], Loss: 2.3410, Perplexity: 10.39212\n",
      "Epoch [1/1], Step [60000/591753], Loss: 2.1931, Perplexity: 8.962768\n",
      "Epoch [1/1], Step [60100/591753], Loss: 2.2309, Perplexity: 9.308278\n",
      "Epoch [1/1], Step [60200/591753], Loss: 2.5640, Perplexity: 12.9872\n",
      "Epoch [1/1], Step [60300/591753], Loss: 1.4161, Perplexity: 4.121000\n",
      "Epoch [1/1], Step [60400/591753], Loss: 3.0050, Perplexity: 20.18597\n",
      "Epoch [1/1], Step [60500/591753], Loss: 2.1279, Perplexity: 8.397541\n",
      "Epoch [1/1], Step [60600/591753], Loss: 2.2044, Perplexity: 9.064954\n",
      "Epoch [1/1], Step [60700/591753], Loss: 0.7900, Perplexity: 2.203458\n",
      "Epoch [1/1], Step [60800/591753], Loss: 1.2979, Perplexity: 3.661885\n",
      "Epoch [1/1], Step [60900/591753], Loss: 2.3921, Perplexity: 10.93672\n",
      "Epoch [1/1], Step [61000/591753], Loss: 1.9229, Perplexity: 6.84079\n",
      "Epoch [1/1], Step [61100/591753], Loss: 1.7303, Perplexity: 5.642221\n",
      "Epoch [1/1], Step [61200/591753], Loss: 1.8275, Perplexity: 6.218285\n",
      "Epoch [1/1], Step [61300/591753], Loss: 1.3173, Perplexity: 3.73341\n",
      "Epoch [1/1], Step [61400/591753], Loss: 2.6565, Perplexity: 14.2463\n",
      "Epoch [1/1], Step [61500/591753], Loss: 2.0048, Perplexity: 7.42456\n",
      "Epoch [1/1], Step [61600/591753], Loss: 3.2214, Perplexity: 25.06332\n",
      "Epoch [1/1], Step [61700/591753], Loss: 1.7815, Perplexity: 5.938679\n",
      "Epoch [1/1], Step [61800/591753], Loss: 2.6666, Perplexity: 14.39124\n",
      "Epoch [1/1], Step [61900/591753], Loss: 1.3561, Perplexity: 3.88098\n",
      "Epoch [1/1], Step [62000/591753], Loss: 1.8591, Perplexity: 6.418274\n",
      "Epoch [1/1], Step [62100/591753], Loss: 0.7044, Perplexity: 2.022733\n",
      "Epoch [1/1], Step [62200/591753], Loss: 5.2325, Perplexity: 187.2697\n",
      "Epoch [1/1], Step [62300/591753], Loss: 1.5336, Perplexity: 4.635025\n",
      "Epoch [1/1], Step [62400/591753], Loss: 1.7439, Perplexity: 5.719835\n",
      "Epoch [1/1], Step [62500/591753], Loss: 1.9954, Perplexity: 7.355395\n",
      "Epoch [1/1], Step [62600/591753], Loss: 2.0193, Perplexity: 7.53305\n",
      "Epoch [1/1], Step [62700/591753], Loss: 2.8502, Perplexity: 17.2907\n",
      "Epoch [1/1], Step [62800/591753], Loss: 1.2973, Perplexity: 3.65955\n",
      "Epoch [1/1], Step [62900/591753], Loss: 2.8653, Perplexity: 17.55396\n",
      "Epoch [1/1], Step [63000/591753], Loss: 3.9142, Perplexity: 50.1104\n",
      "Epoch [1/1], Step [63100/591753], Loss: 2.2676, Perplexity: 9.65643\n",
      "Epoch [1/1], Step [63200/591753], Loss: 1.9189, Perplexity: 6.813447\n",
      "Epoch [1/1], Step [63300/591753], Loss: 2.2807, Perplexity: 9.78382\n",
      "Epoch [1/1], Step [63400/591753], Loss: 2.6891, Perplexity: 14.71782\n",
      "Epoch [1/1], Step [63500/591753], Loss: 2.2715, Perplexity: 9.694151\n",
      "Epoch [1/1], Step [63600/591753], Loss: 2.1489, Perplexity: 8.57572\n",
      "Epoch [1/1], Step [63700/591753], Loss: 2.1649, Perplexity: 8.71359\n",
      "Epoch [1/1], Step [63800/591753], Loss: 2.1372, Perplexity: 8.475713\n",
      "Epoch [1/1], Step [63900/591753], Loss: 3.8298, Perplexity: 46.05564\n",
      "Epoch [1/1], Step [64000/591753], Loss: 1.4944, Perplexity: 4.45651\n",
      "Epoch [1/1], Step [64100/591753], Loss: 1.7821, Perplexity: 5.94258\n",
      "Epoch [1/1], Step [64200/591753], Loss: 2.5656, Perplexity: 13.0091\n",
      "Epoch [1/1], Step [64300/591753], Loss: 3.9096, Perplexity: 49.8811\n",
      "Epoch [1/1], Step [64400/591753], Loss: 2.3763, Perplexity: 10.76519\n",
      "Epoch [1/1], Step [64500/591753], Loss: 2.7792, Perplexity: 16.1056\n",
      "Epoch [1/1], Step [64600/591753], Loss: 2.1173, Perplexity: 8.30888\n",
      "Epoch [1/1], Step [64700/591753], Loss: 2.2686, Perplexity: 9.665442\n",
      "Epoch [1/1], Step [64800/591753], Loss: 2.7476, Perplexity: 15.6054\n",
      "Epoch [1/1], Step [64900/591753], Loss: 2.8318, Perplexity: 16.97621\n",
      "Epoch [1/1], Step [65000/591753], Loss: 2.6425, Perplexity: 14.0484\n",
      "Epoch [1/1], Step [65100/591753], Loss: 2.1865, Perplexity: 8.903970\n",
      "Epoch [1/1], Step [65200/591753], Loss: 2.1253, Perplexity: 8.375112\n",
      "Epoch [1/1], Step [65300/591753], Loss: 1.6132, Perplexity: 5.019098\n",
      "Epoch [1/1], Step [65400/591753], Loss: 2.1764, Perplexity: 8.814532\n",
      "Epoch [1/1], Step [65500/591753], Loss: 2.2426, Perplexity: 9.418049\n",
      "Epoch [1/1], Step [65600/591753], Loss: 2.2208, Perplexity: 9.21444\n",
      "Epoch [1/1], Step [65700/591753], Loss: 1.8067, Perplexity: 6.090489\n",
      "Epoch [1/1], Step [65800/591753], Loss: 3.0017, Perplexity: 20.11948\n",
      "Epoch [1/1], Step [65900/591753], Loss: 1.8102, Perplexity: 6.111775\n",
      "Epoch [1/1], Step [66000/591753], Loss: 2.8072, Perplexity: 16.56415\n",
      "Epoch [1/1], Step [66100/591753], Loss: 1.7308, Perplexity: 5.645460\n",
      "Epoch [1/1], Step [66200/591753], Loss: 2.0068, Perplexity: 7.439616\n",
      "Epoch [1/1], Step [66300/591753], Loss: 2.6876, Perplexity: 14.69594\n",
      "Epoch [1/1], Step [66400/591753], Loss: 1.5166, Perplexity: 4.556638\n",
      "Epoch [1/1], Step [66500/591753], Loss: 2.2854, Perplexity: 9.829395\n",
      "Epoch [1/1], Step [66600/591753], Loss: 1.2280, Perplexity: 3.414496\n",
      "Epoch [1/1], Step [66700/591753], Loss: 2.2553, Perplexity: 9.53801\n",
      "Epoch [1/1], Step [66800/591753], Loss: 2.4617, Perplexity: 11.72471\n",
      "Epoch [1/1], Step [66900/591753], Loss: 3.0675, Perplexity: 21.48744\n",
      "Epoch [1/1], Step [67000/591753], Loss: 2.7452, Perplexity: 15.56804\n",
      "Epoch [1/1], Step [67100/591753], Loss: 2.0039, Perplexity: 7.417611\n",
      "Epoch [1/1], Step [67200/591753], Loss: 1.9008, Perplexity: 6.691032\n",
      "Epoch [1/1], Step [67300/591753], Loss: 3.0469, Perplexity: 21.05002\n",
      "Epoch [1/1], Step [67400/591753], Loss: 2.0340, Perplexity: 7.64427\n",
      "Epoch [1/1], Step [67500/591753], Loss: 2.6577, Perplexity: 14.26303\n",
      "Epoch [1/1], Step [67600/591753], Loss: 3.3574, Perplexity: 28.71336\n",
      "Epoch [1/1], Step [67700/591753], Loss: 2.4020, Perplexity: 11.04475\n",
      "Epoch [1/1], Step [67800/591753], Loss: 2.3464, Perplexity: 10.44766\n",
      "Epoch [1/1], Step [67900/591753], Loss: 4.1514, Perplexity: 63.5255\n",
      "Epoch [1/1], Step [68000/591753], Loss: 3.4274, Perplexity: 30.79702\n",
      "Epoch [1/1], Step [68100/591753], Loss: 2.3480, Perplexity: 10.4645\n",
      "Epoch [1/1], Step [68200/591753], Loss: 1.5685, Perplexity: 4.799435\n",
      "Epoch [1/1], Step [68300/591753], Loss: 1.9258, Perplexity: 6.860994\n",
      "Epoch [1/1], Step [68400/591753], Loss: 1.7401, Perplexity: 5.69774\n",
      "Epoch [1/1], Step [68500/591753], Loss: 2.6541, Perplexity: 14.2115\n",
      "Epoch [1/1], Step [68600/591753], Loss: 2.2325, Perplexity: 9.32328\n",
      "Epoch [1/1], Step [68700/591753], Loss: 2.5301, Perplexity: 12.55530\n",
      "Epoch [1/1], Step [68800/591753], Loss: 3.7082, Perplexity: 40.77946\n",
      "Epoch [1/1], Step [68900/591753], Loss: 2.6021, Perplexity: 13.4926\n",
      "Epoch [1/1], Step [69000/591753], Loss: 3.3656, Perplexity: 28.95120\n",
      "Epoch [1/1], Step [69100/591753], Loss: 4.2486, Perplexity: 70.0042\n",
      "Epoch [1/1], Step [69200/591753], Loss: 1.8649, Perplexity: 6.455094\n",
      "Epoch [1/1], Step [69300/591753], Loss: 1.7481, Perplexity: 5.74386\n",
      "Epoch [1/1], Step [69400/591753], Loss: 1.7646, Perplexity: 5.839571\n",
      "Epoch [1/1], Step [69500/591753], Loss: 2.5098, Perplexity: 12.30234\n",
      "Epoch [1/1], Step [69600/591753], Loss: 1.5126, Perplexity: 4.53879\n",
      "Epoch [1/1], Step [69700/591753], Loss: 3.8231, Perplexity: 45.74501\n",
      "Epoch [1/1], Step [69800/591753], Loss: 2.0300, Perplexity: 7.614484\n",
      "Epoch [1/1], Step [69900/591753], Loss: 2.1280, Perplexity: 8.398213\n",
      "Epoch [1/1], Step [70000/591753], Loss: 2.3640, Perplexity: 10.6337\n",
      "Epoch [1/1], Step [70100/591753], Loss: 2.4490, Perplexity: 11.57734\n",
      "Epoch [1/1], Step [70200/591753], Loss: 2.2092, Perplexity: 9.10856\n",
      "Epoch [1/1], Step [70300/591753], Loss: 3.1196, Perplexity: 22.63741\n",
      "Epoch [1/1], Step [70400/591753], Loss: 1.1978, Perplexity: 3.31276\n",
      "Epoch [1/1], Step [70500/591753], Loss: 0.7364, Perplexity: 2.08847\n",
      "Epoch [1/1], Step [70600/591753], Loss: 2.6001, Perplexity: 13.46485\n",
      "Epoch [1/1], Step [70700/591753], Loss: 3.2677, Perplexity: 26.25064\n",
      "Epoch [1/1], Step [70800/591753], Loss: 1.1559, Perplexity: 3.17693\n",
      "Epoch [1/1], Step [70900/591753], Loss: 2.5706, Perplexity: 13.07346\n",
      "Epoch [1/1], Step [71000/591753], Loss: 2.4517, Perplexity: 11.60847\n",
      "Epoch [1/1], Step [71100/591753], Loss: 1.5569, Perplexity: 4.744122\n",
      "Epoch [1/1], Step [71200/591753], Loss: 1.5016, Perplexity: 4.488857\n",
      "Epoch [1/1], Step [71300/591753], Loss: 1.8758, Perplexity: 6.525719\n",
      "Epoch [1/1], Step [71400/591753], Loss: 2.3075, Perplexity: 10.04911\n",
      "Epoch [1/1], Step [71500/591753], Loss: 1.8679, Perplexity: 6.47493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [71600/591753], Loss: 2.1100, Perplexity: 8.248246\n",
      "Epoch [1/1], Step [71700/591753], Loss: 2.3504, Perplexity: 10.4901\n",
      "Epoch [1/1], Step [71800/591753], Loss: 1.7420, Perplexity: 5.70875\n",
      "Epoch [1/1], Step [71900/591753], Loss: 2.1837, Perplexity: 8.87955\n",
      "Epoch [1/1], Step [72000/591753], Loss: 1.3065, Perplexity: 3.69349\n",
      "Epoch [1/1], Step [72100/591753], Loss: 1.7005, Perplexity: 5.476558\n",
      "Epoch [1/1], Step [72200/591753], Loss: 3.1330, Perplexity: 22.94380\n",
      "Epoch [1/1], Step [72300/591753], Loss: 1.9249, Perplexity: 6.854467\n",
      "Epoch [1/1], Step [72400/591753], Loss: 3.3025, Perplexity: 27.1809\n",
      "Epoch [1/1], Step [72500/591753], Loss: 1.6268, Perplexity: 5.087534\n",
      "Epoch [1/1], Step [72600/591753], Loss: 2.4161, Perplexity: 11.20256\n",
      "Epoch [1/1], Step [72700/591753], Loss: 2.8186, Perplexity: 16.75288\n",
      "Epoch [1/1], Step [72800/591753], Loss: 1.8337, Perplexity: 6.25682\n",
      "Epoch [1/1], Step [72900/591753], Loss: 1.6261, Perplexity: 5.084240\n",
      "Epoch [1/1], Step [73000/591753], Loss: 1.0087, Perplexity: 2.742041\n",
      "Epoch [1/1], Step [73100/591753], Loss: 3.6748, Perplexity: 39.4407\n",
      "Epoch [1/1], Step [73200/591753], Loss: 1.9341, Perplexity: 6.917995\n",
      "Epoch [1/1], Step [73300/591753], Loss: 2.0497, Perplexity: 7.765567\n",
      "Epoch [1/1], Step [73400/591753], Loss: 2.3371, Perplexity: 10.3511\n",
      "Epoch [1/1], Step [73500/591753], Loss: 2.7700, Perplexity: 15.9583\n",
      "Epoch [1/1], Step [73600/591753], Loss: 1.9537, Perplexity: 7.054691\n",
      "Epoch [1/1], Step [73700/591753], Loss: 2.5033, Perplexity: 12.22266\n",
      "Epoch [1/1], Step [73800/591753], Loss: 2.6185, Perplexity: 13.71576\n",
      "Epoch [1/1], Step [73900/591753], Loss: 2.2630, Perplexity: 9.612045\n",
      "Epoch [1/1], Step [74000/591753], Loss: 2.5943, Perplexity: 13.38779\n",
      "Epoch [1/1], Step [74100/591753], Loss: 1.9256, Perplexity: 6.85915\n",
      "Epoch [1/1], Step [74200/591753], Loss: 1.8724, Perplexity: 6.503682\n",
      "Epoch [1/1], Step [74300/591753], Loss: 3.8839, Perplexity: 48.61185\n",
      "Epoch [1/1], Step [74400/591753], Loss: 1.4179, Perplexity: 4.128348\n",
      "Epoch [1/1], Step [74500/591753], Loss: 2.6233, Perplexity: 13.78180\n",
      "Epoch [1/1], Step [74600/591753], Loss: 2.8203, Perplexity: 16.78187\n",
      "Epoch [1/1], Step [74700/591753], Loss: 2.0882, Perplexity: 8.07079\n",
      "Epoch [1/1], Step [74800/591753], Loss: 2.6057, Perplexity: 13.54105\n",
      "Epoch [1/1], Step [74900/591753], Loss: 3.1814, Perplexity: 24.0803\n",
      "Epoch [1/1], Step [75000/591753], Loss: 2.6631, Perplexity: 14.34055\n",
      "Epoch [1/1], Step [75100/591753], Loss: 0.9865, Perplexity: 2.681862\n",
      "Epoch [1/1], Step [75200/591753], Loss: 2.6230, Perplexity: 13.77723\n",
      "Epoch [1/1], Step [75300/591753], Loss: 2.1923, Perplexity: 8.95590\n",
      "Epoch [1/1], Step [75400/591753], Loss: 2.9574, Perplexity: 19.24801\n",
      "Epoch [1/1], Step [75500/591753], Loss: 2.4440, Perplexity: 11.51914\n",
      "Epoch [1/1], Step [75600/591753], Loss: 2.7808, Perplexity: 16.13172\n",
      "Epoch [1/1], Step [75700/591753], Loss: 3.4908, Perplexity: 32.8136\n",
      "Epoch [1/1], Step [75800/591753], Loss: 1.9565, Perplexity: 7.07484\n",
      "Epoch [1/1], Step [75900/591753], Loss: 1.9776, Perplexity: 7.225440\n",
      "Epoch [1/1], Step [76000/591753], Loss: 2.9713, Perplexity: 19.5169\n",
      "Epoch [1/1], Step [76100/591753], Loss: 3.8190, Perplexity: 45.55939\n",
      "Epoch [1/1], Step [76200/591753], Loss: 2.5758, Perplexity: 13.1423\n",
      "Epoch [1/1], Step [76300/591753], Loss: 1.6536, Perplexity: 5.225502\n",
      "Epoch [1/1], Step [76400/591753], Loss: 1.9681, Perplexity: 7.15699\n",
      "Epoch [1/1], Step [76500/591753], Loss: 1.1752, Perplexity: 3.23893\n",
      "Epoch [1/1], Step [76600/591753], Loss: 2.6048, Perplexity: 13.5289\n",
      "Epoch [1/1], Step [76700/591753], Loss: 2.4786, Perplexity: 11.92418\n",
      "Epoch [1/1], Step [76800/591753], Loss: 2.5034, Perplexity: 12.2234\n",
      "Epoch [1/1], Step [76900/591753], Loss: 2.2215, Perplexity: 9.22124\n",
      "Epoch [1/1], Step [77000/591753], Loss: 2.0739, Perplexity: 7.95593\n",
      "Epoch [1/1], Step [77100/591753], Loss: 1.7704, Perplexity: 5.873496\n",
      "Epoch [1/1], Step [77200/591753], Loss: 2.8827, Perplexity: 17.86335\n",
      "Epoch [1/1], Step [77300/591753], Loss: 0.9676, Perplexity: 2.631645\n",
      "Epoch [1/1], Step [77400/591753], Loss: 1.7196, Perplexity: 5.58213\n",
      "Epoch [1/1], Step [77500/591753], Loss: 1.5217, Perplexity: 4.579997\n",
      "Epoch [1/1], Step [77600/591753], Loss: 1.7848, Perplexity: 5.958320\n",
      "Epoch [1/1], Step [77700/591753], Loss: 2.2011, Perplexity: 9.034542\n",
      "Epoch [1/1], Step [77800/591753], Loss: 2.3063, Perplexity: 10.03698\n",
      "Epoch [1/1], Step [77900/591753], Loss: 1.8678, Perplexity: 6.47432\n",
      "Epoch [1/1], Step [78000/591753], Loss: 1.4026, Perplexity: 4.06587\n",
      "Epoch [1/1], Step [78100/591753], Loss: 2.6101, Perplexity: 13.60000\n",
      "Epoch [1/1], Step [78181/591753], Loss: 0.8090, Perplexity: 2.245662"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fc3441728f6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Obtain the batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;31m# Move batch of images and captions to GPU if CUDA is available.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    559\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\PycharmProjects\\firstTest\\Image-Captioning-master\\data_loader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;31m# Convert image to tensor and pre-process using transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'RGB'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \"\"\"\n\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 915\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"P\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deepLearning\\lib\\site-packages\\PIL\\ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m                             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m                             \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "old_time = time.time()\n",
    "# response = requests.request(\"GET\", \n",
    "#                             \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "#                             headers={\"Metadata-Flavor\":\"Google\"})\n",
    "print(device)\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'incremental_comparitive_decoder-%d_four_blocks.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'incremental_comparative_encoder-%d_four_blocks.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a60a0f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14edc084",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f39c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cc30d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc31ecbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9fe074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a07ca0",
   "metadata": {},
   "source": [
    "<h1>SCRATCH PAD</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b350b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "9 2048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# import os\n",
    "# import torch\n",
    "# from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# # TODO #2: Specify the saved models to load.\n",
    "# encoder_file = 'encoder-1.pkl'\n",
    "# decoder_file = 'decoder-1.pkl'\n",
    "\n",
    "# embed_size = 512\n",
    "# hidden_size = 512\n",
    "\n",
    "# encoder = EncoderCNN(embed_size)\n",
    "# encoder.eval()\n",
    "# decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "# decoder.eval()\n",
    "\n",
    "# encoder.to(device)\n",
    "# decoder.to(device)\n",
    "\n",
    "# encoder.load_state_dict(torch.load(os.path.join(os.getcwd(),'models', encoder_file)))\n",
    "# decoder.load_state_dict(torch.load(os.path.join(os.getcwd(),'models', decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "950f0bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "resnet = models.resnet152(pretrained=True)\n",
    "len(list(resnet.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32e80c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(10321, 512)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aa5f7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=512, bias=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(encoder.children())[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154c33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in encoder.parameters():\n",
    "#     print(module.requires_grad)\n",
    "    module.requires_grad = False\n",
    "#     print(module.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9576ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for parameter in decoder.parameters():\n",
    "    print(parameter.requires_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4539b029",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "network_modules = list(encoder.children())\n",
    "resnet_modules = list(network_modules[0].children())\n",
    "print(len(resnet_modules))\n",
    "resnet_modules.pop(7)\n",
    "print(len(resnet_modules))\n",
    "# resnet_modules.pop(-2)\n",
    "# print(len(resnet_modules))\n",
    "# resnet_modules.append(nn.Linear(1024,embed_size))\n",
    "# print(len(resnet_modules))\n",
    "\n",
    "# new_encoder = nn.Sequential(*resnet_modules)\n",
    "# print(new_encoder)\n",
    "# print(len(network_modules))\n",
    "# network_modules = list(network_modules[0]).pop(-2)\n",
    "# print(len(network_modules))\n",
    "# print(len(network_modules))\n",
    "\n",
    "# network_modules = network_modules.append(nn.Conv2d(1024,embed_size))\n",
    "# new_network = nn.Sequential(*network_modules)\n",
    "# new_network\n",
    "# new_network = nn.Sequential(network_modules[0])\n",
    "# print(new_network)\n",
    "# resnet_modules = list(network_modules[0])\n",
    "# print(len(resnet_modules))\n",
    "\n",
    "# for modules in res\n",
    "# resnet_modules.pop(-2)\n",
    "# print(len(resnet_modules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1cf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e571e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c1107b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_4_layered_encoder = nn.Sequential(*resnet_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da616ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "528951cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (23): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (24): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (25): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (26): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (27): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (28): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (29): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (30): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (31): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (32): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (33): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (34): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (35): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (7): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_4_layered_encoder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
